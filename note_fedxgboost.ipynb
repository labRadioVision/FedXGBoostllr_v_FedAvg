{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminaries: choose a dataset and set simulation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.Datasets.data_loader import load_stroke, load_stroke_nprep\n",
    "# from classes.params import simul_param, fl_param\n",
    "\n",
    "\n",
    "data = \"kaggle_stroke\" # stroke data without SMOTE\n",
    "run = 0\n",
    "\n",
    "# load the data (only for centralized perf)\n",
    "if data == \"kaggle_stroke\":\n",
    "    x_train, y_train, x_valid, y_valid = load_stroke()\n",
    "elif data == \"kaggle_stroke_nprep\":\n",
    "    x_train, y_train, x_valid, y_valid = load_stroke_nprep()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets the number of clients and the number of trees (xgboost) per client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clients = 6  # K\n",
    "trees_client = 15  # M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centralized performance\n",
    "Data are fused on the server, this is the classical distributed xboost, privacy critical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Centralized), TPR, TNR: 85.85209 85.80247 85.93482%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['xgb_models/XGB_centralized_model.h5']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "from utils import accuracy\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "hyperparams = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    # Same number of trees as in the decentralized case\n",
    "    \"n_estimators\": num_clients * trees_client,\n",
    "    \"max_depth\": 5,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"base_score\": 0.5,\n",
    "    \"random_state\": 34,\n",
    "}\n",
    "\n",
    "reg = xgb.XGBClassifier(**hyperparams)\n",
    "reg.fit(x_train, y_train)\n",
    "y_pred = reg.predict(x_valid)\n",
    "\n",
    "error_centr = accuracy(y_valid, y_pred)\n",
    "cm = confusion_matrix(y_valid, y_pred)\n",
    "TPR_centralized = cm[1,1]/(cm[1,0] + cm[1,1])\n",
    "TNR_centralized = cm[0,0] / (cm[0,0] + cm[0,1])\n",
    "print(f\"Accuracy (Centralized), TPR, TNR: {100*error_centr :.5f} {100*TPR_centralized :.5f} {100*TNR_centralized :.5f}%\")\n",
    "# save and store the centralized model\n",
    "checkpointpath1 = 'xgb_models/XGB_centralized_model.h5'\n",
    "joblib.dump(reg, checkpointpath1, compress=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INDIVIDUAL CLIENTS (NO FEDERATION)\n",
    "iid split (can be extened with sample/label/feature imbalance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting IID\n",
      "Client 0: KL Divergence: 0.0255 | Wasserstein Distance: 0.0000 | Samples 100\n",
      "Client 1: KL Divergence: 0.0152 | Wasserstein Distance: 0.0000 | Samples 100\n",
      "Client 2: KL Divergence: -0.0340 | Wasserstein Distance: 0.0000 | Samples 100\n",
      "Client 3: KL Divergence: -0.1116 | Wasserstein Distance: 0.0000 | Samples 100\n",
      "Client 4: KL Divergence: 0.1133 | Wasserstein Distance: 0.0000 | Samples 100\n",
      "Client 5: KL Divergence: -0.0050 | Wasserstein Distance: 0.0000 | Samples 100\n",
      "Saved train data\n",
      "Client 0: KL Divergence: -0.0073 | Wasserstein Distance: 0.0000 | Samples 259\n",
      "Client 1: KL Divergence: 0.0043 | Wasserstein Distance: 0.0000 | Samples 259\n",
      "Client 2: KL Divergence: 0.0160 | Wasserstein Distance: 0.0000 | Samples 259\n",
      "Client 3: KL Divergence: 0.0004 | Wasserstein Distance: 0.0000 | Samples 259\n",
      "Client 4: KL Divergence: 0.0082 | Wasserstein Distance: 0.0000 | Samples 259\n",
      "Client 5: KL Divergence: -0.0186 | Wasserstein Distance: 0.0000 | Samples 259\n",
      "Saved valid data\n"
     ]
    }
   ],
   "source": [
    "from classes.Datasets.dataset_client import Dataset\n",
    "from classes.Datasets.data_partitioner import split_iid_sim\n",
    "import os, json\n",
    "# or run python -m classes.Datasets.data_generator.py to get a data distribution\n",
    "samples = 100\n",
    "niid_type = 'iid'\n",
    "alpha = 1\n",
    "\n",
    "print('Splitting IID')\n",
    "\n",
    "# split the training dataset and create folders in data/client_#i/train\n",
    "split_iid_sim(x_train, y_train, samples, type='train')\n",
    "\n",
    "# split the validation dataset and create folders in data/client_#i/valid\n",
    "split_iid_sim(x_valid, y_valid, samples, type='valid')\n",
    "\n",
    "# save data info to json for PS only\n",
    "n_classes = np.unique(y_valid, axis=0).shape[0] if np.unique(y_valid, axis=0).shape[0]>2 else 1\n",
    "data_info = {\n",
    "        'input_shape': x_train.shape[1:],\n",
    "        'num_classes': n_classes, #np.unique(y_valid, axis=0).shape[0],\n",
    "        'data': data,\n",
    "        'niid_type': niid_type,\n",
    "        'alpha': alpha\n",
    "    }\n",
    "# save data/server/\n",
    "dir = \"data/server/\"\n",
    "os.makedirs(dir, exist_ok=True)\n",
    "with open(dir + \"data_info.json\", \"w\") as outfile:\n",
    "    json.dump(data_info, outfile)\n",
    "\n",
    "x_train_clients = []\n",
    "y_train_clients = []\n",
    "x_valid_clients = []\n",
    "y_valid_clients = []\n",
    "\n",
    "# create train and valid datasets for all clients\n",
    "for k in range(num_clients):\n",
    "    handle = Dataset(k) # get an handle to training dataset of client k\n",
    "    x_train_clients.append(handle.x_train_local)\n",
    "    y_train_clients.append(handle.y_train_local)\n",
    "    x_valid_clients.append(handle.x_valid)\n",
    "    y_valid_clients.append(handle.y_valid)\n",
    "\n",
    "datasets = tuple(zip(x_train_clients, y_train_clients))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train the xboost tree models locally. Decision tree models are the ensemble model (base models) for fedxbgoostllr. Save the ensembles and evaluate them separately (no federation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy, TPR, TNR (Client 0): 73.82637 78.80658 65.52316%\n",
      "Accuracy, TPR, TNR (Client 1): 70.54662 73.35391 65.86621%\n",
      "Accuracy, TPR, TNR (Client 2): 72.54019 78.80658 62.09262%\n",
      "Accuracy, TPR, TNR (Client 3): 71.70418 64.71193 83.36192%\n",
      "Accuracy, TPR, TNR (Client 4): 74.21222 81.99588 61.23499%\n",
      "Accuracy, TPR, TNR (Client 5): 69.90354 77.16049 57.80446%\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for each of the clients\n",
    "hyperparams = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"n_estimators\": trees_client,\n",
    "    \"max_depth\": 5,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"base_score\": 0.5,  # np.mean(y_train)\n",
    "    \"random_state\": 34,\n",
    "}\n",
    "\n",
    "errors_clients = []\n",
    "TPR_clients = []\n",
    "TNR_clients = []\n",
    "for c, (x_train, y_train) in enumerate(\n",
    "        datasets\n",
    "):  # extract the dataset for the current client\n",
    "    reg = xgb.XGBClassifier(**hyperparams)\n",
    "    reg.fit(x_train, y_train)\n",
    "    # save model\n",
    "    checkpointpath = 'xgb_models/XGB_client_model_{}.h5'.format(c)\n",
    "    joblib.dump(reg, checkpointpath, compress=0)\n",
    "    # full performance tests (accuracy and confusion matrix)\n",
    "    y_pred = reg.predict(x_valid)\n",
    "\n",
    "    error = accuracy(y_valid, y_pred)\n",
    "    cm = confusion_matrix(y_valid, y_pred)\n",
    "    TPR_isolated = cm[1,1] / (cm[1,0] + cm[1,1])\n",
    "    TNR_isolated = cm[0,0] / (cm[0,0] + cm[0,1])\n",
    "    print(f\"Accuracy, TPR, TNR (Client {c}): {100*error :.5f} {100*TPR_isolated :.5f} {100*TNR_isolated :.5f}%\")\n",
    "    errors_clients.append(error)\n",
    "    TPR_clients.append(TPR_isolated)\n",
    "    TNR_clients.append(TNR_isolated)\n",
    "    # XGB_models.append(reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEDERATED XGBOOST \n",
    "The global model is a 1D-CNN type with specific filter sizes. The global model acts as an \"ensemble model\"\n",
    "\n",
    "Create FIRST a new data for 1D-CNN which consists of XGB trees outputs \n",
    "\n",
    "The pipeline is the following (XGB trees output-> 1D-CNN -> accuracy)\n",
    "\n",
    "DATA: input to FL is now the output of the XGB trees (base models)\n",
    "\n",
    "NOTE: During initialization, all clients xgboost models must be shared among all clients before starting FL process. MQTT can be used for this or models can be loaded from a shared folder. Specific security measures should be probably applied here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting the data of client 0 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 1 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 2 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 3 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 4 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 5 ----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from utils import get_trees_predictions_xgb\n",
    "\n",
    "\n",
    "\n",
    "# load all xgboost models and prepare the data\n",
    "XGB_models = []\n",
    "for c in range(num_clients):\n",
    "    checkpointpath1 = 'xgb_models/XGB_client_model_{}.h5'.format(c)\n",
    "    xgb = joblib.load(checkpointpath1)\n",
    "    XGB_models.append(xgb)\n",
    "\n",
    "# prepare the new dataset for training\n",
    "objective = \"binary\"\n",
    "x_xgb_trees_out = []\n",
    "y_xgb_trees_out = []\n",
    "for c, (x_train, y_train) in enumerate(datasets):  # for each client\n",
    "    print(\"Converting the data of client\", c, 100 * \"-\")\n",
    "    x_xgb_trees_out.append(get_trees_predictions_xgb(x_train, objective, *XGB_models)) # XGB trees outputs (for all XGBoost trees!) corresponding to training data of client c\n",
    "    y_xgb_trees_out.append(y_train) # true labels of client c\n",
    "\n",
    "datasets_out = tuple(zip(x_xgb_trees_out, y_xgb_trees_out)) # dataset_out is the new federated dataset input to 1D-CNN (XGB trees output-> 1D-CNN -> accuracy)\n",
    "\n",
    "# Validation data\n",
    "\n",
    "xgb_valid_out = get_trees_predictions_xgb(x_valid, objective, *XGB_models) # XGB trees outputs corresponding to validation data: to simplify the reasoning, we apply same validation set for all (other options are also feasible)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEDXGBOOST aggregator \n",
    "initialize the global model (or ensemble model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 6, 16)             256       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 96)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 96)                9312      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 97        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9665 (37.75 KB)\n",
      "Trainable params: 9665 (37.75 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from models import CNN\n",
    "\n",
    "filters = 16 # convolutional filters (16, 32 ok, >32 too large, depends on tree structures) TO BE OPTIMIZED\n",
    "filter_size = trees_client # CNN filter size MUST BE equal to the number of trees per client\n",
    "\n",
    "params_cnn = (num_clients, filter_size, filters, objective)\n",
    "models_clients = []  # list of models\n",
    "\n",
    "model_global = CNN(*params_cnn)  # global model\n",
    "num_layers = len(model_global.get_weights())\n",
    "\n",
    "model_global.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEDERATED LEARNING PROCESS \n",
    "Federated averaging with adam optimizer simulator. No MQTT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0/6\n",
      "Round 1/6, Client 1/6\n",
      "Round 1/6, Client 2/6\n",
      "Round 1/6, Client 3/6\n",
      "Round 1/6, Client 4/6\n",
      "Round 1/6, Client 5/6\n",
      "Round 1/6, Client 6/6\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.5755 - accuracy: 0.7344\n",
      "Round 2/6, Client 1/6\n",
      "Round 2/6, Client 2/6\n",
      "Round 2/6, Client 3/6\n",
      "Round 2/6, Client 4/6\n",
      "Round 2/6, Client 5/6\n",
      "Round 2/6, Client 6/6\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.5943 - accuracy: 0.7434\n",
      "Round 3/6, Client 1/6\n",
      "Round 3/6, Client 2/6\n",
      "Round 3/6, Client 3/6\n",
      "Round 3/6, Client 4/6\n",
      "Round 3/6, Client 5/6\n",
      "Round 3/6, Client 6/6\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.5640 - accuracy: 0.7537\n",
      "Round 4/6, Client 1/6\n",
      "Round 4/6, Client 2/6\n",
      "Round 4/6, Client 3/6\n",
      "Round 4/6, Client 4/6\n",
      "Round 4/6, Client 5/6\n",
      "Round 4/6, Client 6/6\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.5659 - accuracy: 0.7524\n",
      "Round 5/6, Client 1/6\n",
      "Round 5/6, Client 2/6\n",
      "Round 5/6, Client 3/6\n",
      "Round 5/6, Client 4/6\n",
      "Round 5/6, Client 5/6\n",
      "Round 5/6, Client 6/6\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.5534 - accuracy: 0.7518\n",
      "Round 6/6, Client 1/6\n",
      "Round 6/6, Client 2/6\n",
      "Round 6/6, Client 3/6\n",
      "Round 6/6, Client 4/6\n",
      "Round 6/6, Client 5/6\n",
      "Round 6/6, Client 6/6\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.5508 - accuracy: 0.7524\n"
     ]
    }
   ],
   "source": [
    "R = 6  # global FL rounds\n",
    "E = 5  # local epochs\n",
    "print(f\"Round 0/{R}\")  # init model\n",
    "\n",
    "for r in range(R):  # for each round\n",
    "    \n",
    "    # update phase for each client\n",
    "    for c, (x_train_c, y_train_c) in enumerate(datasets_out):  \n",
    "        print(f\"Round {r + 1}/{R}, Client {c + 1}/{num_clients}\")\n",
    "        model_client = CNN(*params_cnn)  # create a new model\n",
    "        # set global weights (no memory of prev local weights)\n",
    "        model_client.set_weights(model_global.get_weights())  \n",
    "        # update phase\n",
    "        model_client.fit(\n",
    "            x_train_c, y_train_c, epochs=E, verbose=False\n",
    "        )  # train the model on the client data\n",
    "        models_clients.append(model_client)  # save the model\n",
    "    \n",
    "    # aggregation phase\n",
    "    global_weights = []\n",
    "    for i in range(num_layers):  # aggregate the weights, no memory of prev global weights\n",
    "        global_weights.append(\n",
    "            np.sum([model.get_weights()[i] for model in models_clients], axis=0)\n",
    "            / len(models_clients)\n",
    "        )\n",
    "    model_global.set_weights(global_weights)\n",
    "\n",
    "    model_global.evaluate(xgb_valid_out, y_valid)  # evaluate the global model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final testing on stroke data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 1ms/step\n",
      "Accuracy (Centralized), TPR, TNR: 85.85209 85.80247 85.93482%\n",
      "Accuracy, TPR, TNR: (Client 0): 73.82637 78.80658 65.52316%\n",
      "Accuracy, TPR, TNR: (Client 1): 70.54662 73.35391 65.86621%\n",
      "Accuracy, TPR, TNR: (Client 2): 72.54019 78.80658 62.09262%\n",
      "Accuracy, TPR, TNR: (Client 3): 71.70418 64.71193 83.36192%\n",
      "Accuracy, TPR, TNR: (Client 4): 74.21222 81.99588 61.23499%\n",
      "Accuracy, TPR, TNR: (Client 5): 69.90354 77.16049 57.80446%\n",
      "Accuracy (Federated), TPR, TNR: 75.24116 84.67078 59.51973%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Amministratore\\anaconda3\\envs\\CNRalg4TRUSTroke\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "\n",
    "y_hat_xgbb = model_global.predict(xgb_valid_out)\n",
    "y_hat_xgb = y_hat_xgbb >= 0.5 # binary estimator (CNN model has sigmoid output)\n",
    "\n",
    "error_fed = accuracy(y_valid, y_hat_xgb)\n",
    "\n",
    "# performance and confusion matrix\n",
    "cm = confusion_matrix(y_valid, y_hat_xgb)\n",
    "TPR_fed = cm[1,1] / (cm[1,0] + cm[1,1])\n",
    "TNR_fed = cm[0,0] / (cm[0,0] + cm[0,1])\n",
    "\n",
    "\n",
    "print(f\"Accuracy (Centralized), TPR, TNR: {100*error_centr :.5f} {100*TPR_centralized :.5f} {100*TNR_centralized :.5f}%\")\n",
    "for c, error in enumerate(errors_clients):\n",
    "    print(f\"Accuracy, TPR, TNR: (Client {c}): {100*error :.5f} {100*TPR_clients[c] :.5f} {100*TNR_clients[c] :.5f}%\")\n",
    "print(f\"Accuracy (Federated), TPR, TNR: {100*error_fed :.5f} {100*TPR_fed :.5f} {100*TNR_fed :.5f}%\")\n",
    "\n",
    "# saving results\n",
    "checkpointpath = 'xgb_models/XGB_federated_model.h5'\n",
    "model_global.save(checkpointpath)\n",
    "# joblib.dump(model_global, checkpointpath, compress=0)\n",
    "dict_1 = {\"Accuracy_centralized\": error_centr,\n",
    "          \"TPR_centralized\":  TPR_centralized,\n",
    "          \"TNR_centralized\":  TNR_centralized,\n",
    "          \"Accuracy_clients\": errors_clients,\n",
    "          \"TPR_clients\": TPR_clients,\n",
    "          \"TNR_clients\": TNR_clients,\n",
    "          \"Accuracy_federation\": error_fed,\n",
    "          \"TPR_federation\": TPR_fed,\n",
    "          \"TNR_federation\": TNR_fed,\n",
    "\n",
    "          }\n",
    "sio.savemat(\n",
    "    \"results/fedXGboost_{}_alpha_{}_samples_{}_run_{}.mat\".format('iid',0,100,run), dict_1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgboost",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
