{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHOOSE THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from classes.Datasets.data_loader import load_mnist, load_stroke, load_stroke_nprep\n",
    "from classes.params import simul_param, fl_param\n",
    "from utils import get_trees_predictions_xgb, accuracy, load_unsurance\n",
    "from models import CNN\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import joblib\n",
    "\n",
    "data = \"kaggle_stroke\" # stroke data without SMOTE\n",
    "run = 0\n",
    "\n",
    "# load the data (only for centralized perf)\n",
    "if data == \"kaggle_stroke\":\n",
    "    x_train, y_train, x_valid, y_valid = load_stroke()\n",
    "elif data == \"kaggle_stroke_nprep\":\n",
    "    x_train, y_train, x_valid, y_valid = load_stroke_nprep()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets the number of clients and the number of trees (xgboost) per client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clients = fl_param.NUM_CLIENTS  # K\n",
    "trees_client = 15  # M\n",
    "objective = \"binary\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centralized performance,\n",
    "data are fused on the server, this is the classical distributed xboost, privacy critical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Centralized), TPR, TNR: 85.85209 85.80247 85.93482%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['xgb_models/XGB_centralized_model.h5']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    # Same number of trees as in the decentralized case\n",
    "    \"n_estimators\": num_clients * trees_client,\n",
    "    \"max_depth\": 5,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"base_score\": 0.5,\n",
    "    \"random_state\": 34,\n",
    "}\n",
    "\n",
    "reg = xgb.XGBClassifier(**hyperparams)\n",
    "reg.fit(x_train, y_train)\n",
    "y_pred = reg.predict(x_valid)\n",
    "\n",
    "error_centr = accuracy(y_valid, y_pred)\n",
    "cm = pd.DataFrame(confusion_matrix(y_valid, y_pred)).to_numpy()\n",
    "TPR_centralized = cm[1,1]/(cm[1,0] + cm[1,1])\n",
    "TNR_centralized = cm[0,0] / (cm[0,0] + cm[0,1])\n",
    "print(f\"Accuracy (Centralized), TPR, TNR: {100*error_centr :.5f} {100*TPR_centralized :.5f} {100*TNR_centralized :.5f}%\")\n",
    "# save and store the centralized model\n",
    "checkpointpath1 = 'xgb_models/XGB_centralized_model.h5'\n",
    "joblib.dump(reg, checkpointpath1, compress=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INDIVIDUAL CLIENTS (NO FEDERATION)\n",
    "iid split (can be extened with sample/label/feature imbalance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting IID\n",
      "Client 0: KL Divergence: 0.0255 | Wasserstein Distance: 0.0000 | Samples 100\n",
      "Client 1: KL Divergence: 0.0152 | Wasserstein Distance: 0.0000 | Samples 100\n",
      "Client 2: KL Divergence: -0.0340 | Wasserstein Distance: 0.0000 | Samples 100\n",
      "Client 3: KL Divergence: -0.1116 | Wasserstein Distance: 0.0000 | Samples 100\n",
      "Client 4: KL Divergence: 0.1133 | Wasserstein Distance: 0.0000 | Samples 100\n",
      "Client 5: KL Divergence: -0.0050 | Wasserstein Distance: 0.0000 | Samples 100\n",
      "Saved train data\n",
      "Client 0: KL Divergence: -0.0073 | Wasserstein Distance: 0.0000 | Samples 259\n",
      "Client 1: KL Divergence: 0.0043 | Wasserstein Distance: 0.0000 | Samples 259\n",
      "Client 2: KL Divergence: 0.0160 | Wasserstein Distance: 0.0000 | Samples 259\n",
      "Client 3: KL Divergence: 0.0004 | Wasserstein Distance: 0.0000 | Samples 259\n",
      "Client 4: KL Divergence: 0.0082 | Wasserstein Distance: 0.0000 | Samples 259\n",
      "Client 5: KL Divergence: -0.0186 | Wasserstein Distance: 0.0000 | Samples 259\n",
      "Saved valid data\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 22\u001b[0m\n\u001b[0;32m     14\u001b[0m data_info \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m'\u001b[39m: x_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m'\u001b[39m: n_classes, \u001b[38;5;66;03m#np.unique(y_valid, axis=0).shape[0],\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m: alpha\n\u001b[0;32m     20\u001b[0m     }\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mdir\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/server/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 22\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mdir\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mdir\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_info.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m outfile:\n\u001b[0;32m     24\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(data_info, outfile)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "from classes.Datasets.dataset_client import Dataset\n",
    "from classes.Datasets.data_partitioner import split_iid_sim\n",
    "import os\n",
    "# or run python -m classes.Datasets.data_generator.py to get a data distribution\n",
    "samples = 100\n",
    "niid_type = 'iid'\n",
    "alpha = 1\n",
    "\n",
    "print('Splitting IID')\n",
    "split_iid_sim(x_train, y_train, samples, type='train')\n",
    "# split the validation dataset\n",
    "split_iid_sim(x_valid, y_valid, samples, type='valid')\n",
    "# save data info to json for PS\n",
    "n_classes = np.unique(y_valid, axis=0).shape[0] if np.unique(y_valid, axis=0).shape[0]>2 else 1\n",
    "data_info = {\n",
    "        'input_shape': x_train.shape[1:],\n",
    "        'num_classes': n_classes, #np.unique(y_valid, axis=0).shape[0],\n",
    "        'data': data,\n",
    "        'niid_type': niid_type,\n",
    "        'alpha': alpha\n",
    "    }\n",
    "dir = \"data/server/\"\n",
    "os.makedirs(dir, exist_ok=True)\n",
    "with open(dir + \"data_info.json\", \"w\") as outfile:\n",
    "    json.dump(data_info, outfile)\n",
    "\n",
    "x_train_clients = []\n",
    "y_train_clients = []\n",
    "x_valid_clients = []\n",
    "y_valid_clients = []\n",
    "for k in range(num_clients):\n",
    "    handle = Dataset(k)\n",
    "    x_train_clients.append(handle.x_train_local)\n",
    "    y_train_clients.append(handle.y_train_local)\n",
    "    x_valid_clients.append(handle.x_valid)\n",
    "    y_valid_clients.append(handle.y_valid)\n",
    "\n",
    "datasets = tuple(zip(x_train_clients, y_train_clients))\n",
    "\n",
    "# Hyperparameters for each of the clients\n",
    "hyperparams = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"n_estimators\": trees_client,\n",
    "    \"max_depth\": 5,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"base_score\": 0.5,  # np.mean(y_train)\n",
    "    \"random_state\": 34,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the ensembles and evaluate them separately (no federation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_clients = []\n",
    "TPR_clients = []\n",
    "TNR_clients = []\n",
    "for c, (x_train, y_train) in enumerate(\n",
    "        datasets\n",
    "):  # extract the dataset for the current client\n",
    "    reg = xgb.XGBClassifier(**hyperparams)\n",
    "    reg.fit(x_train, y_train)\n",
    "    # save model\n",
    "    checkpointpath = 'xgb_models/XGB_client_model_{}.h5'.format(c)\n",
    "    joblib.dump(reg, checkpointpath, compress=0)\n",
    "    # full performance tests (accuracy and confusion matrix)\n",
    "    y_pred = reg.predict(x_valid)\n",
    "\n",
    "    error = accuracy(y_valid, y_pred)\n",
    "    cm = pd.DataFrame(confusion_matrix(y_valid, y_pred)).to_numpy()\n",
    "    TPR_isolated = cm[1,1] / (cm[1,0] + cm[1,1])\n",
    "    TNR_isolated = cm[0,0] / (cm[0,0] + cm[0,1])\n",
    "    print(f\"Accuracy, TPR, TNR (Client {c}): {100*error :.5f} {100*TPR_isolated :.5f} {100*TNR_isolated :.5f}%\")\n",
    "    errors_clients.append(error)\n",
    "    TPR_clients.append(TPR_isolated)\n",
    "    TNR_clients.append(TNR_isolated)\n",
    "    # XGB_models.append(reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEDERATED XGBOOST \n",
    "Create FIRST a new data for 1D-CNN (XGB trees output-> 1D-CNN -> accuracy)\n",
    "(DATA is the output of the ensembles, Y: true label)\n",
    "\n",
    "all clients xgboost models must be shared before starting FL process (initializatgion), can be loaded from a shared folder or use MQTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting the data of client 0 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 1 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 2 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 3 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 4 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 5 ----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# load all models and prepare the data\n",
    "XGB_models = []\n",
    "for c in range(num_clients):\n",
    "    checkpointpath1 = 'xgb_models/XGB_client_model_{}.h5'.format(c)\n",
    "    xgb = joblib.load(checkpointpath1)\n",
    "    XGB_models.append(xgb)\n",
    "\n",
    "# prepare the new dataset for training\n",
    "x_data_client_out = []\n",
    "y_data_client_out = []\n",
    "for c, (x_train, y_train) in enumerate(datasets):  # for each client\n",
    "    print(\"Converting the data of client\", c, 100 * \"-\")\n",
    "    x_data_client_out.append(get_trees_predictions_xgb(x_train, objective, *XGB_models))\n",
    "    y_data_client_out.append(y_train)\n",
    "\n",
    "datasets_out = tuple(zip(x_data_client_out, y_data_client_out))\n",
    "\n",
    "# Validation data\n",
    "xgb_valid_out = get_trees_predictions_xgb(x_valid, objective, *XGB_models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEDXGBOOST aggregator \n",
    "initialize the ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_59\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_59 (Conv1D)          (None, 6, 16)             256       \n",
      "                                                                 \n",
      " flatten_59 (Flatten)        (None, 96)                0         \n",
      "                                                                 \n",
      " dense_118 (Dense)           (None, 96)                9312      \n",
      "                                                                 \n",
      " dense_119 (Dense)           (None, 1)                 97        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9665 (37.75 KB)\n",
      "Trainable params: 9665 (37.75 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Round 0/15\n"
     ]
    }
   ],
   "source": [
    "R = 15  # global rounds\n",
    "E = 10  # local epochs\n",
    "filters = 16 # convolutional filters (16 32 ok, not too large)\n",
    "filter_size = trees_client # CNN filter size must be equal to the number of trees per client\n",
    "\n",
    "params_cnn = (num_clients, filter_size, filters, objective)\n",
    "models_clients = []  # list of models\n",
    "\n",
    "model_global = CNN(*params_cnn)  # global model\n",
    "num_layers = len(model_global.get_weights())\n",
    "\n",
    "model_global.summary()\n",
    "print(f\"Round 0/{R}\")  # init model\n",
    "# model_global.evaluate(xgb_valid_out, y_valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEDERATED LEARNING PROCESS \n",
    "(FEDAVG - ADAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1/15, Client 1/6\n",
      "Round 1/15, Client 2/6\n",
      "Round 1/15, Client 3/6\n",
      "Round 1/15, Client 4/6\n",
      "Round 1/15, Client 5/6\n",
      "Round 1/15, Client 6/6\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 0.2007 - accuracy: 0.9417\n",
      "Round 2/15, Client 1/6\n",
      "Round 2/15, Client 2/6\n",
      "Round 2/15, Client 3/6\n",
      "Round 2/15, Client 4/6\n",
      "Round 2/15, Client 5/6\n",
      "Round 2/15, Client 6/6\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.2040 - accuracy: 0.9417\n",
      "Round 3/15, Client 1/6\n",
      "Round 3/15, Client 2/6\n",
      "Round 3/15, Client 3/6\n",
      "Round 3/15, Client 4/6\n",
      "Round 3/15, Client 5/6\n",
      "Round 3/15, Client 6/6\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 0.2053 - accuracy: 0.9417\n",
      "Round 4/15, Client 1/6\n",
      "Round 4/15, Client 2/6\n",
      "Round 4/15, Client 3/6\n",
      "Round 4/15, Client 4/6\n",
      "Round 4/15, Client 5/6\n",
      "Round 4/15, Client 6/6\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 0.2062 - accuracy: 0.9417\n",
      "Round 5/15, Client 1/6\n",
      "Round 5/15, Client 2/6\n",
      "Round 5/15, Client 3/6\n",
      "Round 5/15, Client 4/6\n",
      "Round 5/15, Client 5/6\n",
      "Round 5/15, Client 6/6\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 0.2066 - accuracy: 0.9417\n",
      "Round 6/15, Client 1/6\n",
      "Round 6/15, Client 2/6\n",
      "Round 6/15, Client 3/6\n",
      "Round 6/15, Client 4/6\n",
      "Round 6/15, Client 5/6\n",
      "Round 6/15, Client 6/6\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 0.2096 - accuracy: 0.9417\n",
      "Round 7/15, Client 1/6\n",
      "Round 7/15, Client 2/6\n",
      "Round 7/15, Client 3/6\n",
      "Round 7/15, Client 4/6\n",
      "Round 7/15, Client 5/6\n",
      "Round 7/15, Client 6/6\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 0.2084 - accuracy: 0.9405\n",
      "Round 8/15, Client 1/6\n",
      "Round 8/15, Client 2/6\n",
      "Round 8/15, Client 3/6\n",
      "Round 8/15, Client 4/6\n",
      "Round 8/15, Client 5/6\n",
      "Round 8/15, Client 6/6\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 0.2065 - accuracy: 0.9370\n",
      "Round 9/15, Client 1/6\n",
      "Round 9/15, Client 2/6\n",
      "Round 9/15, Client 3/6\n",
      "Round 9/15, Client 4/6\n",
      "Round 9/15, Client 5/6\n",
      "Round 9/15, Client 6/6\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 0.2079 - accuracy: 0.9393\n",
      "Round 10/15, Client 1/6\n",
      "Round 10/15, Client 2/6\n",
      "Round 10/15, Client 3/6\n",
      "Round 10/15, Client 4/6\n",
      "Round 10/15, Client 5/6\n",
      "Round 10/15, Client 6/6\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.2101 - accuracy: 0.9370\n",
      "Round 11/15, Client 1/6\n",
      "Round 11/15, Client 2/6\n",
      "Round 11/15, Client 3/6\n",
      "Round 11/15, Client 4/6\n",
      "Round 11/15, Client 5/6\n",
      "Round 11/15, Client 6/6\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 0.2085 - accuracy: 0.9370\n",
      "Round 12/15, Client 1/6\n",
      "Round 12/15, Client 2/6\n",
      "Round 12/15, Client 3/6\n",
      "Round 12/15, Client 4/6\n",
      "Round 12/15, Client 5/6\n",
      "Round 12/15, Client 6/6\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 0.2070 - accuracy: 0.9370\n",
      "Round 13/15, Client 1/6\n",
      "Round 13/15, Client 2/6\n",
      "Round 13/15, Client 3/6\n",
      "Round 13/15, Client 4/6\n",
      "Round 13/15, Client 5/6\n",
      "Round 13/15, Client 6/6\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 0.2081 - accuracy: 0.9370\n",
      "Round 14/15, Client 1/6\n",
      "Round 14/15, Client 2/6\n",
      "Round 14/15, Client 3/6\n",
      "Round 14/15, Client 4/6\n",
      "Round 14/15, Client 5/6\n",
      "Round 14/15, Client 6/6\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 0.2101 - accuracy: 0.9370\n",
      "Round 15/15, Client 1/6\n",
      "Round 15/15, Client 2/6\n",
      "Round 15/15, Client 3/6\n",
      "Round 15/15, Client 4/6\n",
      "Round 15/15, Client 5/6\n",
      "Round 15/15, Client 6/6\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 0.2109 - accuracy: 0.9370\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for r in range(R):  # for each round\n",
    "    for c, (x_train_c, y_train_c) in enumerate(datasets_out):  # for each client\n",
    "        print(f\"Round {r + 1}/{R}, Client {c + 1}/{num_clients}\")\n",
    "        model_client = CNN(*params_cnn)  # create a new model\n",
    "        model_client.set_weights(model_global.get_weights())\n",
    "\n",
    "        model_client.fit(\n",
    "            x_train_c, y_train_c, epochs=E, verbose=False\n",
    "        )  # train the model on the client data\n",
    "        models_clients.append(model_client)  # save the model\n",
    "\n",
    "    global_weights = []\n",
    "    for i in range(num_layers):  # aggregate the weights\n",
    "        global_weights.append(\n",
    "            np.sum([model.get_weights()[i] for model in models_clients], axis=0)\n",
    "            / len(models_clients)\n",
    "        )\n",
    "    model_global.set_weights(global_weights)\n",
    "\n",
    "    model_global.evaluate(xgb_valid_out, y_valid)  # evaluate the global model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final testing on stroke data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 2ms/step\n",
      "Accuracy (Centralized), TPR, TNR: 94.39907 99.55947 74.43182%\n",
      "Accuracy, TPR, TNR: (Client 0): 90.54842 93.68576 78.40909%\n",
      "Accuracy, TPR, TNR: (Client 1): 91.94866 95.44787 78.40909%\n",
      "Accuracy, TPR, TNR: (Client 2): 92.64877 98.82526 68.75000%\n",
      "Accuracy, TPR, TNR: (Client 3): 92.88215 96.91630 77.27273%\n",
      "Accuracy, TPR, TNR: (Client 4): 93.58226 97.94420 76.70455%\n",
      "Accuracy, TPR, TNR: (Client 5): 93.46558 98.53157 73.86364%\n",
      "Accuracy (Federated), TPR, TNR: 93.69895 99.26579 72.15909%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ssava\\anaconda3\\envs\\xgboost\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "y_hat_xgbb = model_global.predict(xgb_valid_out)\n",
    "y_hat_xgb = y_hat_xgbb >= 0.5 # binary estimator (CNN model has sigmoid output)\n",
    "\n",
    "error_fed = accuracy(y_valid, y_hat_xgb)\n",
    "\n",
    "# performance and confusion matrix\n",
    "cm = pd.DataFrame(confusion_matrix(y_valid, y_hat_xgb)).to_numpy()\n",
    "TPR_fed = cm[1,1] / (cm[1,0] + cm[1,1])\n",
    "TNR_fed = cm[0,0] / (cm[0,0] + cm[0,1])\n",
    "\n",
    "\n",
    "print(f\"Accuracy (Centralized), TPR, TNR: {100*error_centr :.5f} {100*TPR_centralized :.5f} {100*TNR_centralized :.5f}%\")\n",
    "for c, error in enumerate(errors_clients):\n",
    "    print(f\"Accuracy, TPR, TNR: (Client {c}): {100*error :.5f} {100*TPR_clients[c] :.5f} {100*TNR_clients[c] :.5f}%\")\n",
    "print(f\"Accuracy (Federated), TPR, TNR: {100*error_fed :.5f} {100*TPR_fed :.5f} {100*TNR_fed :.5f}%\")\n",
    "\n",
    "# saving results\n",
    "checkpointpath = 'xgb_models/XGB_federated_model.h5'\n",
    "model_global.save(checkpointpath)\n",
    "# joblib.dump(model_global, checkpointpath, compress=0)\n",
    "dict_1 = {\"Accuracy_centralized\": error_centr,\n",
    "          \"TPR_centralized\":  TPR_centralized,\n",
    "          \"TNR_centralized\":  TNR_centralized,\n",
    "          \"Accuracy_clients\": errors_clients,\n",
    "          \"TPR_clients\": TPR_clients,\n",
    "          \"TNR_clients\": TNR_clients,\n",
    "          \"Accuracy_federation\": error_fed,\n",
    "          \"TPR_federation\": TPR_fed,\n",
    "          \"TNR_federation\": TNR_fed,\n",
    "\n",
    "          }\n",
    "sio.savemat(\n",
    "    \"results/fedXGboost_{}_alpha_{}_samples_{}_run_{}.mat\".format('iid',0,100,run), dict_1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgboost",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
