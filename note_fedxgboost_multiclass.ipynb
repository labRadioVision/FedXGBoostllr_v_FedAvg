{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries \n",
    "Choose a dataset and set simulation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from classes.params import simul_param, fl_param\n",
    "\n",
    "# load a tabular dataset for multiclass classification (example with scikitlearn datasets)\n",
    "num_classes = 4\n",
    "X, y = datasets.make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=4)\n",
    "x_train, x_valid,  y_train, y_valid = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets the number of clients and the number of trees (xgboost) per client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clients = 5  # K\n",
    "trees_client = 10  # M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centralized performance\n",
    "Data are fused on the server, this is the classical distributed xboost, privacy critical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.72\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['xgb_models/XGB_centralized_model.h5']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "from utils import accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "# xgboost parameters (example)\n",
    "hyperparams = {\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    # Same number of trees as in the decentralized case\n",
    "    \"n_estimators\": num_clients * trees_client,\n",
    "    \"max_depth\": 5,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"base_score\": 0.5,\n",
    "    \"random_state\": 34,\n",
    "}\n",
    "\n",
    "reg = xgb.XGBClassifier(**hyperparams)\n",
    "reg.fit(x_train, y_train)\n",
    "y_pred = reg.predict(x_valid)\n",
    "accuracy_s = accuracy_score(y_valid, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_s:.2f}\") \n",
    "\n",
    "cm = confusion_matrix(y_valid, y_pred)\n",
    "# save and store the centralized model\n",
    "checkpointpath1 = 'xgb_models/XGB_centralized_model.h5'\n",
    "joblib.dump(reg, checkpointpath1, compress=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolated case (no federation) \n",
    "Training of local xgboost models (base models of the ensemble)\n",
    "\n",
    "Code below implements iid split (can be extened with sample/label/feature imbalance), saves training, validation data in data/client_i and server parameters in server folders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting IID\n",
      "Client 0 | Samples 100\n",
      "Client 1 | Samples 100\n",
      "Client 2 | Samples 100\n",
      "Client 3 | Samples 100\n",
      "Client 4 | Samples 100\n",
      "Saved train data\n",
      "Client 0 | Samples 80\n",
      "Client 1 | Samples 80\n",
      "Client 2 | Samples 80\n",
      "Client 3 | Samples 80\n",
      "Client 4 | Samples 80\n",
      "Saved valid data\n"
     ]
    }
   ],
   "source": [
    "from classes.Datasets.dataset_client import Dataset\n",
    "from classes.Datasets.data_partitioner import split_iid_sim\n",
    "import os, json\n",
    "# or run python -m classes.Datasets.data_generator.py to get a data distribution\n",
    "samples = 100\n",
    "niid_type = 'iid'\n",
    "alpha = 1\n",
    "\n",
    "print('Splitting IID')\n",
    "\n",
    "# split the training dataset and create folders in data/client_#i/train\n",
    "split_iid_sim(x_train, y_train, samples, num_clients, type='train')\n",
    "\n",
    "# split the validation dataset and create folders in data/client_#i/valid\n",
    "split_iid_sim(x_valid, y_valid, samples, num_clients, type='valid')\n",
    "\n",
    "x_train_clients = []\n",
    "y_train_clients = []\n",
    "x_valid_clients = []\n",
    "y_valid_clients = []\n",
    "\n",
    "# create train and valid datasets for all clients\n",
    "for k in range(num_clients):\n",
    "    handle = Dataset(k) # get an handle to training dataset of client k\n",
    "    x_train_clients.append(handle.x_train_local)\n",
    "    y_train_clients.append(handle.y_train_local)\n",
    "    x_valid_clients.append(handle.x_valid)\n",
    "    y_valid_clients.append(handle.y_valid)\n",
    "\n",
    "datasets = tuple(zip(x_train_clients, y_train_clients))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the xboost tree models locally. Decision tree models are the ensemble model (base models) for fedxbgoostllr. Save the ensembles and evaluate them separately (no federation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost classifier local model accuracy, (Client 0): 60.75000%\n",
      "xgboost classifier local model accuracy, (Client 1): 52.25000%\n",
      "xgboost classifier local model accuracy, (Client 2): 56.75000%\n",
      "xgboost classifier local model accuracy, (Client 3): 54.00000%\n",
      "xgboost classifier local model accuracy, (Client 4): 52.75000%\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for each of the clients\n",
    "hyperparams = {\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"n_estimators\": trees_client,\n",
    "    \"max_depth\": 5,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"base_score\": 0.5,  # np.mean(y_train)\n",
    "    \"random_state\": 34,\n",
    "}\n",
    "\n",
    "errors_clients = []\n",
    "\n",
    "for c, (x_train, y_train) in enumerate(\n",
    "        datasets\n",
    "):  # extract the dataset for the current client\n",
    "    reg = xgb.XGBClassifier(**hyperparams) # train the classifier\n",
    "    reg.fit(x_train, y_train)\n",
    "    # save model\n",
    "    checkpointpath = 'xgb_models/XGB_client_model_{}.h5'.format(c)\n",
    "    joblib.dump(reg, checkpointpath, compress=0)\n",
    "    \n",
    "    # full performance tests (accuracy and confusion matrix)\n",
    "    y_pred = reg.predict(x_valid)\n",
    "    error = accuracy_score(y_valid, y_pred)\n",
    "    cm = confusion_matrix(y_valid, y_pred)\n",
    "    print(f\"xgboost classifier local model accuracy, (Client {c}): {100*error :.5f}%\")\n",
    "    errors_clients.append(error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated XGBoost \n",
    "The global model is a 1D-CNN type with specific filter sizes. The global model acts as an \"ensemble model\"\n",
    "\n",
    "The pipeline is the following (XGB trees outputs-> 1D-CNN -> predictions)\n",
    "\n",
    "In the following the local xgboost models are now re-trained to solve a one vs all classification problem. The outputs of the local xgboost models are treated as inputs to the 1D-CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One vs all local model accuracy, (Client 0): 59.50000%\n",
      "One vs all local model accuracy, (Client 1): 54.50000%\n",
      "One vs all local model accuracy, (Client 2): 55.00000%\n",
      "One vs all local model accuracy, (Client 3): 55.75000%\n",
      "One vs all local model accuracy, (Client 4): 57.50000%\n"
     ]
    }
   ],
   "source": [
    "################################# INDIVIDUAL CLIENTS XGBOOST REGRESSION MODEL TRAINING)\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Hyperparameters for each of the clients\n",
    "hyperparams = {\n",
    "    # \"objective\": \"reg:squarederror\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"n_estimators\": trees_client,\n",
    "    \"max_depth\": 5,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"base_score\": 0.5,  # np.mean(y_train)\n",
    "    \"random_state\": 34,\n",
    "}\n",
    "\n",
    "for c, (x_train, y_train) in enumerate(\n",
    "        datasets\n",
    "):  # extract the dataset for the current client\n",
    "    reg = OneVsRestClassifier(xgb.XGBClassifier(**hyperparams)) # regression model training\n",
    "    reg.fit(x_train, y_train)\n",
    "    # save model\n",
    "    checkpointpath = 'xgb_models/XGB_client_model_fed_{}.h5'.format(c)\n",
    "    joblib.dump(reg, checkpointpath, compress=0)\n",
    "  \n",
    "# test of the one vs all local model (compared with the previous case)\n",
    "    y_pred = reg.predict(x_valid)\n",
    "    error = accuracy_score(y_valid, y_pred)\n",
    "    cm = confusion_matrix(y_valid, y_pred)\n",
    "    print(f\"One vs all local model accuracy, (Client {c}): {100*error :.5f}%\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new \"dataset\" input to 1D-CNN which consists of the XGB trees regression models previously trained (locally):\n",
    "\n",
    "NOTE: During initialization, all xgboost models (of all clients) must be shared with all clients before starting the FL process. MQTT can be used for this (but also other methods apply). In the following xgboost base models are loaded from a shared folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting the data of client 0 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 1 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 2 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 3 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 4 ----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from utils import get_trees_predictions_xgb\n",
    "\n",
    "# load all xgboost models and prepare the data\n",
    "XGB_models = []\n",
    "for c in range(num_clients):\n",
    "    checkpointpath1 = 'xgb_models/XGB_client_model_fed_{}.h5'.format(c)\n",
    "    xgb = joblib.load(checkpointpath1)\n",
    "    classifiers = xgb.estimators_\n",
    "    for q in range(num_classes):\n",
    "        XGB_models.append(classifiers [q])\n",
    "\n",
    "reshape_enabled = False # disable or enable the reshaping of xgboost outputs\n",
    "# prepare the new dataset for training\n",
    "objective = \"binary\" \n",
    "# other options: \n",
    "# objective = \"soft\" # applies a tanh activation to the xgboost tree soft outputs  \n",
    "# objective = \"binary\" # outputs of xgboost trees are binarized, \n",
    "# binary option seems working slightly better but unclear\n",
    "x_xgb_trees_out = []\n",
    "y_xgb_trees_out = []\n",
    "for c, (x_train, y_train) in enumerate(datasets):  # for each client\n",
    "    print(\"Converting the data of client\", c, 100 * \"-\")\n",
    "    # XGB trees outputs (for all XGBoost trees!) corresponding to training data of client c\n",
    "    # NOTE: numclasses is needed to clip the inputs\n",
    "    x_xgb_trees_out.append(get_trees_predictions_xgb(x_train, objective, *XGB_models, numclasses=num_classes, reshape_enabled=reshape_enabled)) \n",
    "    categorical_labels = np.squeeze(np.eye(num_classes)[y_train.reshape(-1)])\n",
    "    y_xgb_trees_out.append(categorical_labels) # true labels now categorical\n",
    "\n",
    "datasets_out = tuple(zip(x_xgb_trees_out, y_xgb_trees_out)) # dataset_out is the new federated dataset input to 1D-CNN (XGB trees output-> 1D-CNN -> accuracy)\n",
    "\n",
    "# Validation data\n",
    "\n",
    "xgb_valid_out = get_trees_predictions_xgb(x_valid, objective, *XGB_models, numclasses=num_classes, reshape_enabled=reshape_enabled) # XGB trees outputs corresponding to validation data: to simplify the reasoning, we apply same validation set for all (other options are also feasible)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FedXGBooost aggregator \n",
    "initialize the global model (or ensemble model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_378\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_378 (Conv1D)         (None, 17, 32)            1312      \n",
      "                                                                 \n",
      " flatten_378 (Flatten)       (None, 544)               0         \n",
      "                                                                 \n",
      " dense_756 (Dense)           (None, 160)               87200     \n",
      "                                                                 \n",
      " dense_757 (Dense)           (None, 4)                 644       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 89156 (348.27 KB)\n",
      "Trainable params: 89156 (348.27 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from models import CNN_mc\n",
    "\n",
    "filters = 32 # convolutional filters (32 ok, >32 too large, depends on tree structures) TO BE OPTIMIZED\n",
    "if reshape_enabled:\n",
    "    filter_size = trees_client\n",
    "else:\n",
    "    filter_size = trees_client * num_classes # CNN filter size equal to the number of trees per client * number of classes (if > 2)\n",
    "\n",
    "params_cnn = (num_clients, filter_size, trees_client, filters, num_classes)\n",
    "models_clients = []  # list of models\n",
    "\n",
    "model_global = CNN_mc(*params_cnn)  # global model\n",
    "num_layers = len(model_global.get_weights())\n",
    "\n",
    "model_global.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning process \n",
    "Federated Averaging with Adam optimizer simulator. No MQTT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0/25\n",
      "Round 1/25, Client 1/5\n",
      "Round 1/25, Client 2/5\n",
      "Round 1/25, Client 3/5\n",
      "Round 1/25, Client 4/5\n",
      "Round 1/25, Client 5/5\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.2667 - accuracy: 0.6425\n",
      "Round 2/25, Client 1/5\n",
      "Round 2/25, Client 2/5\n",
      "Round 2/25, Client 3/5\n",
      "Round 2/25, Client 4/5\n",
      "Round 2/25, Client 5/5\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4061 - accuracy: 0.6500\n",
      "Round 3/25, Client 1/5\n",
      "Round 3/25, Client 2/5\n",
      "Round 3/25, Client 3/5\n",
      "Round 3/25, Client 4/5\n",
      "Round 3/25, Client 5/5\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4391 - accuracy: 0.6650\n",
      "Round 4/25, Client 1/5\n",
      "Round 4/25, Client 2/5\n",
      "Round 4/25, Client 3/5\n",
      "Round 4/25, Client 4/5\n",
      "Round 4/25, Client 5/5\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4302 - accuracy: 0.6675\n",
      "Round 5/25, Client 1/5\n",
      "Round 5/25, Client 2/5\n",
      "Round 5/25, Client 3/5\n",
      "Round 5/25, Client 4/5\n",
      "Round 5/25, Client 5/5\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.4540 - accuracy: 0.6750\n",
      "Round 6/25, Client 1/5\n",
      "Round 6/25, Client 2/5\n",
      "Round 6/25, Client 3/5\n",
      "Round 6/25, Client 4/5\n",
      "Round 6/25, Client 5/5\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1.4682 - accuracy: 0.6700\n",
      "Round 7/25, Client 1/5\n",
      "Round 7/25, Client 2/5\n",
      "Round 7/25, Client 3/5\n",
      "Round 7/25, Client 4/5\n",
      "Round 7/25, Client 5/5\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.4790 - accuracy: 0.6750\n",
      "Round 8/25, Client 1/5\n",
      "Round 8/25, Client 2/5\n",
      "Round 8/25, Client 3/5\n",
      "Round 8/25, Client 4/5\n",
      "Round 8/25, Client 5/5\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.4937 - accuracy: 0.6725\n",
      "Round 9/25, Client 1/5\n",
      "Round 9/25, Client 2/5\n",
      "Round 9/25, Client 3/5\n",
      "Round 9/25, Client 4/5\n",
      "Round 9/25, Client 5/5\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.5107 - accuracy: 0.6700\n",
      "Round 10/25, Client 1/5\n",
      "Round 10/25, Client 2/5\n",
      "Round 10/25, Client 3/5\n",
      "Round 10/25, Client 4/5\n",
      "Round 10/25, Client 5/5\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.5185 - accuracy: 0.6725\n",
      "Round 11/25, Client 1/5\n",
      "Round 11/25, Client 2/5\n",
      "Round 11/25, Client 3/5\n",
      "Round 11/25, Client 4/5\n",
      "Round 11/25, Client 5/5\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.5165 - accuracy: 0.6725\n",
      "Round 12/25, Client 1/5\n",
      "Round 12/25, Client 2/5\n",
      "Round 12/25, Client 3/5\n",
      "Round 12/25, Client 4/5\n",
      "Round 12/25, Client 5/5\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.5174 - accuracy: 0.6800\n",
      "Round 13/25, Client 1/5\n",
      "Round 13/25, Client 2/5\n",
      "Round 13/25, Client 3/5\n",
      "Round 13/25, Client 4/5\n",
      "Round 13/25, Client 5/5\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1.5227 - accuracy: 0.6775\n",
      "Round 14/25, Client 1/5\n",
      "Round 14/25, Client 2/5\n",
      "Round 14/25, Client 3/5\n",
      "Round 14/25, Client 4/5\n",
      "Round 14/25, Client 5/5\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.5271 - accuracy: 0.6775\n",
      "Round 15/25, Client 1/5\n",
      "Round 15/25, Client 2/5\n",
      "Round 15/25, Client 3/5\n",
      "Round 15/25, Client 4/5\n",
      "Round 15/25, Client 5/5\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.5333 - accuracy: 0.6825\n",
      "Round 16/25, Client 1/5\n",
      "Round 16/25, Client 2/5\n",
      "Round 16/25, Client 3/5\n",
      "Round 16/25, Client 4/5\n",
      "Round 16/25, Client 5/5\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.5333 - accuracy: 0.6800\n",
      "Round 17/25, Client 1/5\n",
      "Round 17/25, Client 2/5\n",
      "Round 17/25, Client 3/5\n",
      "Round 17/25, Client 4/5\n",
      "Round 17/25, Client 5/5\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.5314 - accuracy: 0.6825\n",
      "Round 18/25, Client 1/5\n",
      "Round 18/25, Client 2/5\n",
      "Round 18/25, Client 3/5\n",
      "Round 18/25, Client 4/5\n",
      "Round 18/25, Client 5/5\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.5347 - accuracy: 0.6825\n",
      "Round 19/25, Client 1/5\n",
      "Round 19/25, Client 2/5\n",
      "Round 19/25, Client 3/5\n",
      "Round 19/25, Client 4/5\n",
      "Round 19/25, Client 5/5\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.5332 - accuracy: 0.6825\n",
      "Round 20/25, Client 1/5\n",
      "Round 20/25, Client 2/5\n",
      "Round 20/25, Client 3/5\n",
      "Round 20/25, Client 4/5\n",
      "Round 20/25, Client 5/5\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.5357 - accuracy: 0.6750\n",
      "Round 21/25, Client 1/5\n",
      "Round 21/25, Client 2/5\n",
      "Round 21/25, Client 3/5\n",
      "Round 21/25, Client 4/5\n",
      "Round 21/25, Client 5/5\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.5324 - accuracy: 0.6775\n",
      "Round 22/25, Client 1/5\n",
      "Round 22/25, Client 2/5\n",
      "Round 22/25, Client 3/5\n",
      "Round 22/25, Client 4/5\n",
      "Round 22/25, Client 5/5\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.5318 - accuracy: 0.6775\n",
      "Round 23/25, Client 1/5\n",
      "Round 23/25, Client 2/5\n",
      "Round 23/25, Client 3/5\n",
      "Round 23/25, Client 4/5\n",
      "Round 23/25, Client 5/5\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.5374 - accuracy: 0.6775\n",
      "Round 24/25, Client 1/5\n",
      "Round 24/25, Client 2/5\n",
      "Round 24/25, Client 3/5\n",
      "Round 24/25, Client 4/5\n",
      "Round 24/25, Client 5/5\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.5392 - accuracy: 0.6750\n",
      "Round 25/25, Client 1/5\n",
      "Round 25/25, Client 2/5\n",
      "Round 25/25, Client 3/5\n",
      "Round 25/25, Client 4/5\n",
      "Round 25/25, Client 5/5\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.5383 - accuracy: 0.6725\n"
     ]
    }
   ],
   "source": [
    "R = 25  # global FL rounds\n",
    "E = 10  # local epochs\n",
    "\n",
    "print(f\"Round 0/{R}\")  # init model\n",
    "\n",
    "# sets labels to categorical\n",
    "categorical_y_valid = np.squeeze(np.eye(num_classes)[y_valid.reshape(-1)])\n",
    "\n",
    "for r in range(R):  # for each round\n",
    "    \n",
    "    # update phase for each client\n",
    "    for c, (x_train_c, y_train_c) in enumerate(datasets_out):  \n",
    "        print(f\"Round {r + 1}/{R}, Client {c + 1}/{num_clients}\")\n",
    "        model_client = CNN_mc(*params_cnn)  # create a new model\n",
    "        # set global weights (no memory of prev local weights)\n",
    "        model_client.set_weights(model_global.get_weights())  \n",
    "        # update phase\n",
    "        model_client.fit(\n",
    "            x_train_c, y_train_c, epochs=E, verbose=False\n",
    "        )  # train the model on the client data\n",
    "        models_clients.append(model_client)  # save the model\n",
    "    \n",
    "    # aggregation phase\n",
    "    global_weights = []\n",
    "    for i in range(num_layers):  # aggregate the weights, no memory of prev global weights\n",
    "        global_weights.append(\n",
    "            np.sum([model.get_weights()[i] for model in models_clients], axis=0)\n",
    "            / len(models_clients)\n",
    "        )\n",
    "    model_global.set_weights(global_weights)\n",
    "\n",
    "    model_global.evaluate(xgb_valid_out, categorical_y_valid)  # evaluate the global model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 4ms/step\n",
      "Accuracy (Centralized): 0.72\n",
      "Accuracy (Client 0): 0.61\n",
      "Accuracy (Client 1): 0.52\n",
      "Accuracy (Client 2): 0.57\n",
      "Accuracy (Client 3): 0.54\n",
      "Accuracy (Client 4): 0.53\n",
      "Accuracy (Federated): 0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\xgboost_fed\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "\n",
    "y_hat_xgb = model_global.predict(xgb_valid_out)\n",
    "y_hat_xgb_cont = np.argmax(y_hat_xgb, axis=1)\n",
    "accuracy_fed = accuracy_score(y_valid, y_hat_xgb_cont)\n",
    "cm = confusion_matrix(y_valid, y_hat_xgb_cont)\n",
    "\n",
    "# performance and confusion matrix\n",
    "\n",
    "print(f\"Accuracy (Centralized): {accuracy_s :.2f}\")\n",
    "for c, error in enumerate(errors_clients):\n",
    "    print(f\"Accuracy (Client {c}): {error :.2f}\")\n",
    "print(f\"Accuracy (Federated): {accuracy_fed :.2f}\")\n",
    "\n",
    "# saving results\n",
    "checkpointpath = 'xgb_models/XGB_federated_model_regression_multiclass.h5'\n",
    "model_global.save(checkpointpath)\n",
    "\n",
    "# sio.savemat(\n",
    "#    \"results/fedXGboost_{}_alpha_{}_samples_{}_run_{}.mat\".format('iid',0,100,run), dict_1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgboost_fed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
