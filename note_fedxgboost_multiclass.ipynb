{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries \n",
    "Choose a dataset and set simulation parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "num_classes = 4 # number of classes (>2)\n",
    "n_features = 50 # number of features\n",
    "n_redundant = 15 # redundant features\n",
    "n_informative = n_features - n_redundant # informative features\n",
    "test_size = 0.4 # fraction of data used for validation\n",
    "training_samples = 1000\n",
    "n_samples = round(training_samples/(1-test_size))\n",
    "random_state = 42\n",
    "# load a tabular dataset for multiclass classification (example with scikitlearn datasets)\n",
    "X, y = datasets.make_classification(n_samples=n_samples, n_features=n_features, n_informative=n_informative, n_redundant=n_redundant, n_classes=num_classes)\n",
    "x_train, x_valid,  y_train, y_valid = train_test_split(X, y, test_size=test_size, random_state=random_state)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets the number of clients, the training samples per client and the number of trees (xgboost) per client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clients = 8  # number of FL clients\n",
    "trees_client = 10  # number of xgboost trees per client\n",
    "samples = round(n_samples/num_clients) # number of training examples per client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centralized performance\n",
    "Data are fused on the server, this is the classical distributed xboost, privacy critical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.68\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['xgb_models/XGB_centralized_model.h5']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "from utils import accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "# xgboost parameters (example)\n",
    "hyperparams = {\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    # Same number of trees as in the decentralized case\n",
    "    \"n_estimators\": num_clients * trees_client,\n",
    "    \"max_depth\": 5,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"base_score\": 0.5,\n",
    "    \"random_state\": 34,\n",
    "}\n",
    "\n",
    "reg = xgb.XGBClassifier(**hyperparams)\n",
    "reg.fit(x_train, y_train)\n",
    "y_pred = reg.predict(x_valid)\n",
    "accuracy_s = accuracy_score(y_valid, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_s:.2f}\") \n",
    "\n",
    "cm = confusion_matrix(y_valid, y_pred)\n",
    "# save and store the centralized model\n",
    "checkpointpath1 = 'xgb_models/XGB_centralized_model.h5'\n",
    "joblib.dump(reg, checkpointpath1, compress=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolated case (no federation) \n",
    "Training of local xgboost models (base models of the ensemble)\n",
    "\n",
    "Code below implements iid/uniform data split among the deployed clients. It can be extened including sample/label/feature imbalance. Training and validation data are saved in different folders, namely data/client_i/train and data/client_i/validation. Parameter server parameters are also saved in the server folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting IID\n",
      "Client 0 | Samples 125\n",
      "Client 1 | Samples 125\n",
      "Client 2 | Samples 125\n",
      "Client 3 | Samples 125\n",
      "Client 4 | Samples 125\n",
      "Client 5 | Samples 125\n",
      "Client 6 | Samples 125\n",
      "Client 7 | Samples 125\n",
      "Saved train data\n",
      "Client 0 | Samples 667\n",
      "Client 1 | Samples 667\n",
      "Client 2 | Samples 667\n",
      "Client 3 | Samples 667\n",
      "Client 4 | Samples 667\n",
      "Client 5 | Samples 667\n",
      "Client 6 | Samples 667\n",
      "Client 7 | Samples 667\n",
      "Saved validation data\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "\n",
    "niid_type = 'iid'\n",
    "alpha = 1\n",
    "\n",
    "print('Splitting IID')\n",
    "local_size = samples  # uniform split, and a assign 'samples' samples\n",
    "# split the training dataset and create folders in data/client_#i/train\n",
    "for i in range(num_clients):\n",
    "    dir = f'data/client_{i}/train/' # create a folder with the local data for the client\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    start_index = i * local_size\n",
    "    end_index = (i + 1) * local_size\n",
    "    x_part = x_train[start_index:end_index]\n",
    "    y_part = y_train[start_index:end_index]\n",
    "        \n",
    "    print('Client {} | Samples {}'.format(i, len(y_part)))\n",
    "    np.save(dir + f'x_train.npy', x_part) # creating directories for train and validation\n",
    "    np.save(dir + f'y_train.npy', y_part)\n",
    "print(f'Saved train data')\n",
    "\n",
    "# split the validation dataset and create folders in data/client_#i/valid\n",
    "local_size = len(x_valid) // num_clients # validation data uniformly distributed across clients (other options are also possible) \n",
    "# local_size = len(x_valid)\n",
    "for i in range(num_clients):\n",
    "    dir = f'data/client_{i}/valid/' # create a folder with the local data for the client\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    start_index = i * local_size\n",
    "    end_index = (i + 1) * local_size\n",
    "    #x_part = x_valid[start_index:end_index] # uniform split of the validation set\n",
    "    #y_part = y_valid[start_index:end_index]\n",
    "    x_part = x_valid # all the clients have the same validation set (for fair comparison)\n",
    "    y_part = y_valid\n",
    "        \n",
    "    print('Client {} | Samples {}'.format(i, len(y_part)))\n",
    "    np.save(dir + f'x_valid.npy', x_part) # saving\n",
    "    np.save(dir + f'y_valid.npy', y_part)\n",
    "print(f'Saved validation data')\n",
    "\n",
    "\n",
    "x_train_clients = []\n",
    "y_train_clients = []\n",
    "x_valid_clients = []\n",
    "y_valid_clients = []\n",
    "\n",
    "# create train and valid datasets for all clients\n",
    "for k in range(num_clients):\n",
    "    x_train_clients.append(np.load(f'data/client_{k}/train/x_train.npy', allow_pickle=True))\n",
    "    y_train_clients.append(np.load(f'data/client_{k}/train/y_train.npy'))\n",
    "    x_valid_clients.append(np.load(f'data/client_{k}/valid/x_valid.npy', allow_pickle=True))\n",
    "    y_valid_clients.append(np.load(f'data/client_{k}/valid/y_valid.npy'))\n",
    "\n",
    "datasets = tuple(zip(x_train_clients, y_train_clients))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the xboost tree models locally. Decision tree models are the ensemble model (base models) for fedxbgoostllr. Save the ensembles and evaluate them separately (no federation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost classifier local model accuracy, (Client 0): 37.33133%\n",
      "xgboost classifier local model accuracy, (Client 1): 44.37781%\n",
      "xgboost classifier local model accuracy, (Client 2): 38.83058%\n",
      "xgboost classifier local model accuracy, (Client 3): 43.77811%\n",
      "xgboost classifier local model accuracy, (Client 4): 38.83058%\n",
      "xgboost classifier local model accuracy, (Client 5): 40.92954%\n",
      "xgboost classifier local model accuracy, (Client 6): 41.07946%\n",
      "xgboost classifier local model accuracy, (Client 7): 48.27586%\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for each of the clients\n",
    "hyperparams = {\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"n_estimators\": trees_client,\n",
    "    \"max_depth\": 5,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"base_score\": 0.5,  # np.mean(y_train)\n",
    "    \"random_state\": 34,\n",
    "}\n",
    "\n",
    "errors_clients = []\n",
    "\n",
    "for c, (x_train, y_train) in enumerate(\n",
    "        datasets\n",
    "):  # extract the dataset for the current client\n",
    "    reg = xgb.XGBClassifier(**hyperparams) # train the classifier\n",
    "    reg.fit(x_train, y_train)\n",
    "    # save model\n",
    "    checkpointpath = 'xgb_models/XGB_client_model_{}.h5'.format(c)\n",
    "    joblib.dump(reg, checkpointpath, compress=0)\n",
    "    \n",
    "    # full performance tests (accuracy and confusion matrix)\n",
    "    y_pred = reg.predict(x_valid)\n",
    "    error = accuracy_score(y_valid, y_pred)\n",
    "    cm = confusion_matrix(y_valid, y_pred)\n",
    "    print(f\"xgboost classifier local model accuracy, (Client {c}): {100*error :.5f}%\")\n",
    "    errors_clients.append(error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated XGBoost \n",
    "The global model is a 1D-CNN type with specific filter sizes. The global model acts as an \"ensemble model\"\n",
    "\n",
    "The pipeline is the following (XGB trees outputs-> 1D-CNN -> predictions)\n",
    "\n",
    "In the following the local xgboost models are now re-trained to solve a one vs all classification problem. The outputs of the local xgboost models are treated as inputs to the 1D-CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One vs all local model accuracy, (Client 0): 40.62969%\n",
      "One vs all local model accuracy, (Client 1): 46.47676%\n",
      "One vs all local model accuracy, (Client 2): 42.57871%\n",
      "One vs all local model accuracy, (Client 3): 44.97751%\n",
      "One vs all local model accuracy, (Client 4): 38.53073%\n",
      "One vs all local model accuracy, (Client 5): 43.92804%\n",
      "One vs all local model accuracy, (Client 6): 39.88006%\n",
      "One vs all local model accuracy, (Client 7): 45.72714%\n"
     ]
    }
   ],
   "source": [
    "################################# INDIVIDUAL CLIENTS XGBOOST REGRESSION MODEL TRAINING)\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Hyperparameters for each of the clients\n",
    "hyperparams = {\n",
    "    # \"objective\": \"reg:squarederror\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"n_estimators\": trees_client,\n",
    "    \"max_depth\": 5,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"base_score\": 0.5,  # np.mean(y_train)\n",
    "    \"random_state\": 34,\n",
    "}\n",
    "\n",
    "for c, (x_train, y_train) in enumerate(\n",
    "        datasets\n",
    "):  # extract the dataset for the current client\n",
    "    reg = OneVsRestClassifier(xgb.XGBClassifier(**hyperparams)) # regression model training\n",
    "    reg.fit(x_train, y_train)\n",
    "    # save model\n",
    "    checkpointpath = 'xgb_models/XGB_client_model_fed_{}.h5'.format(c)\n",
    "    joblib.dump(reg, checkpointpath, compress=0)\n",
    "  \n",
    "# test of the one vs all local model (compared with the previous case)\n",
    "    y_pred = reg.predict(x_valid)\n",
    "    error = accuracy_score(y_valid, y_pred)\n",
    "    cm = confusion_matrix(y_valid, y_pred)\n",
    "    print(f\"One vs all local model accuracy, (Client {c}): {100*error :.5f}%\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new \"dataset\" input to 1D-CNN which consists of the XGB trees regression models previously trained (locally):\n",
    "\n",
    "NOTE: During initialization, all xgboost models (of all clients) must be shared with all clients before starting the FL process. MQTT can be used for this (but also other methods apply). In the following xgboost base models are loaded from a shared folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting the data of client 0 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 1 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 2 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 3 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 4 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 5 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 6 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 7 ----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from utils import get_trees_predictions_xgb\n",
    "\n",
    "# load all xgboost models and prepare the data\n",
    "XGB_models = []\n",
    "for c in range(num_clients):\n",
    "    checkpointpath1 = 'xgb_models/XGB_client_model_fed_{}.h5'.format(c)\n",
    "    xgb = joblib.load(checkpointpath1)\n",
    "    classifiers = xgb.estimators_\n",
    "    for q in range(num_classes):\n",
    "        XGB_models.append(classifiers [q])\n",
    "\n",
    "reshape_enabled = True # disable or enable the reshaping of xgboost outputs (false seems better)\n",
    "# prepare the new dataset for training\n",
    "objective = \"binary\" \n",
    "#objective = \"soft\"\n",
    "# other options: \n",
    "# objective = \"soft\" # applies a tanh activation to the xgboost tree soft outputs  \n",
    "# objective = \"binary\" # outputs of xgboost trees are binarized, \n",
    "# binary option seems working slightly better but unclear\n",
    "x_xgb_trees_out = []\n",
    "y_xgb_trees_out = []\n",
    "for c, (x_train, y_train) in enumerate(datasets):  # for each client\n",
    "    print(\"Converting the data of client\", c, 100 * \"-\")\n",
    "    # XGB trees outputs (for all XGBoost trees!) corresponding to training data of client c\n",
    "    # NOTE: numclasses is needed to clip the inputs\n",
    "    x_xgb_trees_out.append(get_trees_predictions_xgb(x_train, objective, *XGB_models, numclasses=num_classes, reshape_enabled=reshape_enabled)) \n",
    "    categorical_labels = np.squeeze(np.eye(num_classes)[y_train.reshape(-1)])\n",
    "    y_xgb_trees_out.append(categorical_labels) # true labels now categorical\n",
    "\n",
    "datasets_out = tuple(zip(x_xgb_trees_out, y_xgb_trees_out)) # dataset_out is the new federated dataset input to 1D-CNN (XGB trees output-> 1D-CNN -> accuracy)\n",
    "\n",
    "# Validation data\n",
    "\n",
    "xgb_valid_out = get_trees_predictions_xgb(x_valid, objective, *XGB_models, numclasses=num_classes, reshape_enabled=reshape_enabled) # XGB trees outputs corresponding to validation data: to simplify the reasoning, we apply same validation set for all (other options are also feasible)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FedXGBooost aggregator \n",
    "initialize the global model (or ensemble model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1511\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_1511 (Conv1D)        (None, 32, 32)            352       \n",
      "                                                                 \n",
      " flatten_1511 (Flatten)      (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_3021 (Dense)          (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_3022 (Dense)          (None, 4)                 1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 263780 (1.01 MB)\n",
      "Trainable params: 263780 (1.01 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from models import CNN_mc # check the model in models.py\n",
    "\n",
    "filters = 32 # convolutional filters (32 ok, >32 too large, depends on tree structures) TO BE OPTIMIZED\n",
    "if reshape_enabled:\n",
    "    filter_size = trees_client\n",
    "else:\n",
    "    filter_size = trees_client * num_classes # CNN filter size equal to the number of trees per client * number of classes (if > 2)\n",
    "\n",
    "params_cnn = (num_clients, filter_size, trees_client, filters, num_classes)\n",
    "models_clients = []  # list of models\n",
    "\n",
    "model_global = CNN_mc(*params_cnn)  # global model\n",
    "num_layers = len(model_global.get_weights())\n",
    "\n",
    "model_global.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning process \n",
    "Federated Averaging with Adam optimizer simulator. No MQTT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0/25\n",
      "Round 1/25, Client 1/8\n",
      "Round 1/25, Client 2/8\n",
      "Round 1/25, Client 3/8\n",
      "Round 1/25, Client 4/8\n",
      "Round 1/25, Client 5/8\n",
      "Round 1/25, Client 6/8\n",
      "Round 1/25, Client 7/8\n",
      "Round 1/25, Client 8/8\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 1.0726 - accuracy: 0.5562\n",
      "Round 2/25, Client 1/8\n",
      "Round 2/25, Client 2/8\n",
      "Round 2/25, Client 3/8\n",
      "Round 2/25, Client 4/8\n",
      "Round 2/25, Client 5/8\n",
      "Round 2/25, Client 6/8\n",
      "Round 2/25, Client 7/8\n",
      "Round 2/25, Client 8/8\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 1.0348 - accuracy: 0.5877\n",
      "Round 3/25, Client 1/8\n",
      "Round 3/25, Client 2/8\n",
      "Round 3/25, Client 3/8\n",
      "Round 3/25, Client 4/8\n",
      "Round 3/25, Client 5/8\n",
      "Round 3/25, Client 6/8\n",
      "Round 3/25, Client 7/8\n",
      "Round 3/25, Client 8/8\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 1.0626 - accuracy: 0.5952\n",
      "Round 4/25, Client 1/8\n",
      "Round 4/25, Client 2/8\n",
      "Round 4/25, Client 3/8\n",
      "Round 4/25, Client 4/8\n",
      "Round 4/25, Client 5/8\n",
      "Round 4/25, Client 6/8\n",
      "Round 4/25, Client 7/8\n",
      "Round 4/25, Client 8/8\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 1.0719 - accuracy: 0.6027\n",
      "Round 5/25, Client 1/8\n",
      "Round 5/25, Client 2/8\n",
      "Round 5/25, Client 3/8\n",
      "Round 5/25, Client 4/8\n",
      "Round 5/25, Client 5/8\n",
      "Round 5/25, Client 6/8\n",
      "Round 5/25, Client 7/8\n",
      "Round 5/25, Client 8/8\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 1.0969 - accuracy: 0.6027\n",
      "Round 6/25, Client 1/8\n",
      "Round 6/25, Client 2/8\n",
      "Round 6/25, Client 3/8\n",
      "Round 6/25, Client 4/8\n",
      "Round 6/25, Client 5/8\n",
      "Round 6/25, Client 6/8\n",
      "Round 6/25, Client 7/8\n",
      "Round 6/25, Client 8/8\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 1.1143 - accuracy: 0.6027\n",
      "Round 7/25, Client 1/8\n",
      "Round 7/25, Client 2/8\n",
      "Round 7/25, Client 3/8\n",
      "Round 7/25, Client 4/8\n",
      "Round 7/25, Client 5/8\n",
      "Round 7/25, Client 6/8\n",
      "Round 7/25, Client 7/8\n",
      "Round 7/25, Client 8/8\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 1.1249 - accuracy: 0.6042\n",
      "Round 8/25, Client 1/8\n",
      "Round 8/25, Client 2/8\n",
      "Round 8/25, Client 3/8\n",
      "Round 8/25, Client 4/8\n",
      "Round 8/25, Client 5/8\n",
      "Round 8/25, Client 6/8\n",
      "Round 8/25, Client 7/8\n",
      "Round 8/25, Client 8/8\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 1.1351 - accuracy: 0.6057\n",
      "Round 9/25, Client 1/8\n",
      "Round 9/25, Client 2/8\n",
      "Round 9/25, Client 3/8\n",
      "Round 9/25, Client 4/8\n",
      "Round 9/25, Client 5/8\n",
      "Round 9/25, Client 6/8\n",
      "Round 9/25, Client 7/8\n",
      "Round 9/25, Client 8/8\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 1.1423 - accuracy: 0.6072\n",
      "Round 10/25, Client 1/8\n",
      "Round 10/25, Client 2/8\n",
      "Round 10/25, Client 3/8\n",
      "Round 10/25, Client 4/8\n",
      "Round 10/25, Client 5/8\n",
      "Round 10/25, Client 6/8\n",
      "Round 10/25, Client 7/8\n",
      "Round 10/25, Client 8/8\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 1.1478 - accuracy: 0.6087\n",
      "Round 11/25, Client 1/8\n",
      "Round 11/25, Client 2/8\n",
      "Round 11/25, Client 3/8\n",
      "Round 11/25, Client 4/8\n",
      "Round 11/25, Client 5/8\n",
      "Round 11/25, Client 6/8\n",
      "Round 11/25, Client 7/8\n",
      "Round 11/25, Client 8/8\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 1.1515 - accuracy: 0.6102\n",
      "Round 12/25, Client 1/8\n",
      "Round 12/25, Client 2/8\n",
      "Round 12/25, Client 3/8\n",
      "Round 12/25, Client 4/8\n",
      "Round 12/25, Client 5/8\n",
      "Round 12/25, Client 6/8\n",
      "Round 12/25, Client 7/8\n",
      "Round 12/25, Client 8/8\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 1.1546 - accuracy: 0.6117\n",
      "Round 13/25, Client 1/8\n",
      "Round 13/25, Client 2/8\n",
      "Round 13/25, Client 3/8\n",
      "Round 13/25, Client 4/8\n",
      "Round 13/25, Client 5/8\n",
      "Round 13/25, Client 6/8\n",
      "Round 13/25, Client 7/8\n",
      "Round 13/25, Client 8/8\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 1.1600 - accuracy: 0.6102\n",
      "Round 14/25, Client 1/8\n",
      "Round 14/25, Client 2/8\n",
      "Round 14/25, Client 3/8\n",
      "Round 14/25, Client 4/8\n",
      "Round 14/25, Client 5/8\n",
      "Round 14/25, Client 6/8\n",
      "Round 14/25, Client 7/8\n",
      "Round 14/25, Client 8/8\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 1.1642 - accuracy: 0.6087\n",
      "Round 15/25, Client 1/8\n",
      "Round 15/25, Client 2/8\n",
      "Round 15/25, Client 3/8\n",
      "Round 15/25, Client 4/8\n",
      "Round 15/25, Client 5/8\n",
      "Round 15/25, Client 6/8\n",
      "Round 15/25, Client 7/8\n",
      "Round 15/25, Client 8/8\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 1.1680 - accuracy: 0.6102\n",
      "Round 16/25, Client 1/8\n",
      "Round 16/25, Client 2/8\n",
      "Round 16/25, Client 3/8\n",
      "Round 16/25, Client 4/8\n",
      "Round 16/25, Client 5/8\n",
      "Round 16/25, Client 6/8\n",
      "Round 16/25, Client 7/8\n",
      "Round 16/25, Client 8/8\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 1.1737 - accuracy: 0.6117\n",
      "Round 17/25, Client 1/8\n",
      "Round 17/25, Client 2/8\n",
      "Round 17/25, Client 3/8\n",
      "Round 17/25, Client 4/8\n",
      "Round 17/25, Client 5/8\n",
      "Round 17/25, Client 6/8\n",
      "Round 17/25, Client 7/8\n",
      "Round 17/25, Client 8/8\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 1.1779 - accuracy: 0.6132\n",
      "Round 18/25, Client 1/8\n",
      "Round 18/25, Client 2/8\n",
      "Round 18/25, Client 3/8\n",
      "Round 18/25, Client 4/8\n",
      "Round 18/25, Client 5/8\n",
      "Round 18/25, Client 6/8\n",
      "Round 18/25, Client 7/8\n",
      "Round 18/25, Client 8/8\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 1.1798 - accuracy: 0.6132\n",
      "Round 19/25, Client 1/8\n",
      "Round 19/25, Client 2/8\n",
      "Round 19/25, Client 3/8\n",
      "Round 19/25, Client 4/8\n",
      "Round 19/25, Client 5/8\n",
      "Round 19/25, Client 6/8\n",
      "Round 19/25, Client 7/8\n",
      "Round 19/25, Client 8/8\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 1.1835 - accuracy: 0.6147\n",
      "Round 20/25, Client 1/8\n",
      "Round 20/25, Client 2/8\n",
      "Round 20/25, Client 3/8\n",
      "Round 20/25, Client 4/8\n",
      "Round 20/25, Client 5/8\n",
      "Round 20/25, Client 6/8\n",
      "Round 20/25, Client 7/8\n",
      "Round 20/25, Client 8/8\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 1.1842 - accuracy: 0.6132\n",
      "Round 21/25, Client 1/8\n",
      "Round 21/25, Client 2/8\n",
      "Round 21/25, Client 3/8\n",
      "Round 21/25, Client 4/8\n",
      "Round 21/25, Client 5/8\n",
      "Round 21/25, Client 6/8\n",
      "Round 21/25, Client 7/8\n",
      "Round 21/25, Client 8/8\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 1.1867 - accuracy: 0.6117\n",
      "Round 22/25, Client 1/8\n",
      "Round 22/25, Client 2/8\n",
      "Round 22/25, Client 3/8\n",
      "Round 22/25, Client 4/8\n",
      "Round 22/25, Client 5/8\n",
      "Round 22/25, Client 6/8\n",
      "Round 22/25, Client 7/8\n",
      "Round 22/25, Client 8/8\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 1.1893 - accuracy: 0.6117\n",
      "Round 23/25, Client 1/8\n",
      "Round 23/25, Client 2/8\n",
      "Round 23/25, Client 3/8\n",
      "Round 23/25, Client 4/8\n",
      "Round 23/25, Client 5/8\n",
      "Round 23/25, Client 6/8\n",
      "Round 23/25, Client 7/8\n",
      "Round 23/25, Client 8/8\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 1.1925 - accuracy: 0.6147\n",
      "Round 24/25, Client 1/8\n",
      "Round 24/25, Client 2/8\n",
      "Round 24/25, Client 3/8\n",
      "Round 24/25, Client 4/8\n",
      "Round 24/25, Client 5/8\n",
      "Round 24/25, Client 6/8\n",
      "Round 24/25, Client 7/8\n",
      "Round 24/25, Client 8/8\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 1.1934 - accuracy: 0.6162\n",
      "Round 25/25, Client 1/8\n",
      "Round 25/25, Client 2/8\n",
      "Round 25/25, Client 3/8\n",
      "Round 25/25, Client 4/8\n",
      "Round 25/25, Client 5/8\n",
      "Round 25/25, Client 6/8\n",
      "Round 25/25, Client 7/8\n",
      "Round 25/25, Client 8/8\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 1.1946 - accuracy: 0.6162\n"
     ]
    }
   ],
   "source": [
    "R = 25  # global FL rounds\n",
    "E = 10  # local epochs\n",
    "\n",
    "print(f\"Round 0/{R}\")  # init model\n",
    "\n",
    "# sets labels to categorical\n",
    "categorical_y_valid = np.squeeze(np.eye(num_classes)[y_valid.reshape(-1)])\n",
    "\n",
    "for r in range(R):  # for each round\n",
    "    \n",
    "    # update phase for each client\n",
    "    for c, (x_train_c, y_train_c) in enumerate(datasets_out):  \n",
    "        print(f\"Round {r + 1}/{R}, Client {c + 1}/{num_clients}\")\n",
    "        model_client = CNN_mc(*params_cnn)  # create a new model\n",
    "        # set global weights (no memory of prev local weights)\n",
    "        model_client.set_weights(model_global.get_weights())  \n",
    "        # update phase\n",
    "        model_client.fit(\n",
    "            x_train_c, y_train_c, epochs=E, verbose=False\n",
    "        )  # train the model on the client data\n",
    "        models_clients.append(model_client)  # save the model\n",
    "    \n",
    "    # aggregation phase\n",
    "    global_weights = []\n",
    "    for i in range(num_layers):  # aggregate the weights, no memory of prev global weights\n",
    "        global_weights.append(\n",
    "            np.sum([model.get_weights()[i] for model in models_clients], axis=0)\n",
    "            / len(models_clients)\n",
    "        )\n",
    "    model_global.set_weights(global_weights)\n",
    "\n",
    "    model_global.evaluate(xgb_valid_out, categorical_y_valid)  # evaluate the global model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 2ms/step\n",
      "Accuracy (Centralized): 0.68\n",
      "Accuracy (Client 0): 0.37\n",
      "Accuracy (Client 1): 0.44\n",
      "Accuracy (Client 2): 0.39\n",
      "Accuracy (Client 3): 0.44\n",
      "Accuracy (Client 4): 0.39\n",
      "Accuracy (Client 5): 0.41\n",
      "Accuracy (Client 6): 0.41\n",
      "Accuracy (Client 7): 0.48\n",
      "Accuracy (Federated): 0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Amministratore\\anaconda3\\envs\\CNRalg4TRUSTroke\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "\n",
    "y_hat_xgb = model_global.predict(xgb_valid_out)\n",
    "y_hat_xgb_cont = np.argmax(y_hat_xgb, axis=1)\n",
    "accuracy_fed = accuracy_score(y_valid, y_hat_xgb_cont)\n",
    "cm = confusion_matrix(y_valid, y_hat_xgb_cont)\n",
    "\n",
    "# performance and confusion matrix\n",
    "\n",
    "print(f\"Accuracy (Centralized): {accuracy_s :.2f}\")\n",
    "for c, error in enumerate(errors_clients):\n",
    "    print(f\"Accuracy (Client {c}): {error :.2f}\")\n",
    "print(f\"Accuracy (Federated): {accuracy_fed :.2f}\")\n",
    "\n",
    "# saving results\n",
    "checkpointpath = 'xgb_models/XGB_federated_model_regression_multiclass.h5'\n",
    "model_global.save(checkpointpath)\n",
    "dict_1 = {\n",
    "    \"Accuracy_centralized\": accuracy_s,\n",
    "    \"Accuracy_clients\": errors_clients,\n",
    "    \"Accuracy_federated\": accuracy_fed\n",
    "}\n",
    "sio.savemat(\n",
    "    \"results/fedXGboost_{}_features_{}_redundant_{}_classes_{}_clients_{}_trees_client_{}_train_samples_{}_reshape_{}_objective{}.mat\".format('iid',n_features,n_reduntant,num_classes, num_clients, trees_client, samples, reshape_enabled, objective), dict_1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CNRalg4TRUSTroke",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
