{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries \n",
    "Choose a dataset and set simulation parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "num_classes = 4 # number of classes (>2)\n",
    "n_features = 50 # number of features\n",
    "n_redundant = 25 # redundant features\n",
    "n_informative = n_features - n_redundant # informative features\n",
    "test_size = 0.4 # fraction of data used for validation\n",
    "training_samples = 1000\n",
    "n_samples = round(training_samples/(1-test_size))\n",
    "random_state = 42\n",
    "# load a tabular dataset for multiclass classification (example with scikitlearn datasets)\n",
    "X, y = datasets.make_classification(n_samples=n_samples, n_features=n_features, n_informative=n_informative, n_redundant=n_redundant, n_classes=num_classes)\n",
    "x_train, x_valid,  y_train, y_valid = train_test_split(X, y, test_size=test_size, random_state=random_state)    \n",
    "with open(f\"dataset/dataset_{num_classes}_redundant_{n_redundant}.pkl\", \"wb\") as f:\n",
    "    pickle.dump([x_train, x_valid,  y_train, y_valid], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets the number of clients, the training samples per client and the number of trees (xgboost) per client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 4 # number of classes (>2)\n",
    "n_redundant = 15 # redundant features\n",
    "num_clients = 10  # number of FL clients\n",
    "trees_client = 10  # number of xgboost trees per client\n",
    "samples = round(training_samples/num_clients) # number of training examples per client\n",
    "\n",
    "# load the dataset\n",
    "with open(f\"dataset/dataset_{num_classes}_redundant_{n_redundant}.pkl\", 'rb') as f:\n",
    "    x_train, x_valid,  y_train, y_valid = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centralized performance\n",
    "Data are fused on the server, this is the classical distributed xboost, privacy critical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.69\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['xgb_models/XGB_centralized_model.h5']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "from utils import accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "# xgboost parameters (example)\n",
    "hyperparams = {\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    # Same number of trees as in the decentralized case\n",
    "    \"n_estimators\": num_clients * trees_client,\n",
    "    \"max_depth\": 5,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"base_score\": 0.5,\n",
    "    \"random_state\": 34,\n",
    "}\n",
    "if num_classes ==2:\n",
    "    hyperparams[\"objective\"] = \"binary:logistic\"\n",
    "    \n",
    "reg = xgb.XGBClassifier(**hyperparams)\n",
    "reg.fit(x_train, y_train)\n",
    "y_pred = reg.predict(x_valid)\n",
    "accuracy_s = accuracy_score(y_valid, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_s:.2f}\") \n",
    "\n",
    "cm = confusion_matrix(y_valid, y_pred)\n",
    "# save and store the centralized model\n",
    "checkpointpath1 = 'xgb_models/XGB_centralized_model.h5'\n",
    "joblib.dump(reg, checkpointpath1, compress=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolated case (no federation) \n",
    "Training of local xgboost models (base models of the ensemble)\n",
    "\n",
    "Code below implements iid/uniform data split among the deployed clients. It can be extened including sample/label/feature imbalance. Training and validation data are saved in different folders, namely data/client_i/train and data/client_i/validation. Parameter server parameters are also saved in the server folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting IID\n",
      "Client 0 | Samples 100\n",
      "Client 1 | Samples 100\n",
      "Client 2 | Samples 100\n",
      "Client 3 | Samples 100\n",
      "Client 4 | Samples 100\n",
      "Client 5 | Samples 100\n",
      "Client 6 | Samples 100\n",
      "Client 7 | Samples 100\n",
      "Client 8 | Samples 100\n",
      "Client 9 | Samples 100\n",
      "Saved train data\n",
      "Client 0 | Samples 667\n",
      "Client 1 | Samples 667\n",
      "Client 2 | Samples 667\n",
      "Client 3 | Samples 667\n",
      "Client 4 | Samples 667\n",
      "Client 5 | Samples 667\n",
      "Client 6 | Samples 667\n",
      "Client 7 | Samples 667\n",
      "Client 8 | Samples 667\n",
      "Client 9 | Samples 667\n",
      "Saved validation data\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "\n",
    "print('Splitting IID')\n",
    "local_size = samples  # uniform split, and a assign 'samples' samples\n",
    "# split the training dataset and create folders in data/client_#i/train\n",
    "for i in range(num_clients):\n",
    "    dir = f'data/client_{i}/train/' # create a folder with the local data for the client\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    start_index = i * local_size\n",
    "    end_index = (i + 1) * local_size\n",
    "    x_part = x_train[start_index:end_index]\n",
    "    y_part = y_train[start_index:end_index]\n",
    "        \n",
    "    print('Client {} | Samples {}'.format(i, len(y_part)))\n",
    "    np.save(dir + f'x_train.npy', x_part) # creating directories for train and validation\n",
    "    np.save(dir + f'y_train.npy', y_part)\n",
    "print(f'Saved train data')\n",
    "\n",
    "# split the validation dataset and create folders in data/client_#i/valid\n",
    "local_size = len(x_valid) // num_clients # validation data uniformly distributed across clients (other options are also possible) \n",
    "# local_size = len(x_valid)\n",
    "for i in range(num_clients):\n",
    "    dir = f'data/client_{i}/valid/' # create a folder with the local data for the client\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    start_index = i * local_size\n",
    "    end_index = (i + 1) * local_size\n",
    "    #x_part = x_valid[start_index:end_index] # uniform split of the validation set\n",
    "    #y_part = y_valid[start_index:end_index]\n",
    "    x_part = x_valid # all the clients have the same validation set (for fair comparison)\n",
    "    y_part = y_valid\n",
    "        \n",
    "    print('Client {} | Samples {}'.format(i, len(y_part)))\n",
    "    np.save(dir + f'x_valid.npy', x_part) # saving\n",
    "    np.save(dir + f'y_valid.npy', y_part)\n",
    "print(f'Saved validation data')\n",
    "\n",
    "\n",
    "x_train_clients = []\n",
    "y_train_clients = []\n",
    "x_valid_clients = []\n",
    "y_valid_clients = []\n",
    "\n",
    "# create train and valid datasets for all clients\n",
    "for k in range(num_clients):\n",
    "    x_train_clients.append(np.load(f'data/client_{k}/train/x_train.npy', allow_pickle=True))\n",
    "    y_train_clients.append(np.load(f'data/client_{k}/train/y_train.npy'))\n",
    "    x_valid_clients.append(np.load(f'data/client_{k}/valid/x_valid.npy', allow_pickle=True))\n",
    "    y_valid_clients.append(np.load(f'data/client_{k}/valid/y_valid.npy'))\n",
    "\n",
    "datasets = tuple(zip(x_train_clients, y_train_clients))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the xboost tree models locally. Decision tree models are the ensemble model (base models) for fedxbgoostllr. Save the ensembles and evaluate them separately (no federation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost classifier local model accuracy, (Client 0): 37.93103%\n",
      "xgboost classifier local model accuracy, (Client 1): 37.03148%\n",
      "xgboost classifier local model accuracy, (Client 2): 42.87856%\n",
      "xgboost classifier local model accuracy, (Client 3): 40.02999%\n",
      "xgboost classifier local model accuracy, (Client 4): 37.78111%\n",
      "xgboost classifier local model accuracy, (Client 5): 35.53223%\n",
      "xgboost classifier local model accuracy, (Client 6): 42.57871%\n",
      "xgboost classifier local model accuracy, (Client 7): 42.27886%\n",
      "xgboost classifier local model accuracy, (Client 8): 39.13043%\n",
      "xgboost classifier local model accuracy, (Client 9): 44.67766%\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for each of the clients\n",
    "hyperparams = {\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"n_estimators\": trees_client,\n",
    "    \"max_depth\": 5,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"base_score\": 0.5,  # np.mean(y_train)\n",
    "    \"random_state\": 34,\n",
    "}\n",
    "if num_classes ==2:\n",
    "    hyperparams[\"objective\"] = \"binary:logistic\"\n",
    "    \n",
    "errors_clients = []\n",
    "\n",
    "for c, (x_train, y_train) in enumerate(\n",
    "        datasets\n",
    "):  # extract the dataset for the current client\n",
    "    reg = xgb.XGBClassifier(**hyperparams) # train the classifier\n",
    "    reg.fit(x_train, y_train)\n",
    "    # save model\n",
    "    checkpointpath = 'xgb_models/XGB_client_model_{}.h5'.format(c)\n",
    "    joblib.dump(reg, checkpointpath, compress=0)\n",
    "    \n",
    "    # full performance tests (accuracy and confusion matrix)\n",
    "    y_pred = reg.predict(x_valid)\n",
    "    error = accuracy_score(y_valid, y_pred)\n",
    "    cm = confusion_matrix(y_valid, y_pred)\n",
    "    print(f\"xgboost classifier local model accuracy, (Client {c}): {100*error :.5f}%\")\n",
    "    errors_clients.append(error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated XGBoost \n",
    "The global model is a 1D-CNN type with specific filter sizes. The global model acts as an \"ensemble model\"\n",
    "\n",
    "The pipeline is the following (XGB trees outputs-> 1D-CNN -> predictions)\n",
    "\n",
    "In the following the local xgboost models are now re-trained to solve a one vs all classification problem. The outputs of the local xgboost models are treated as inputs to the 1D-CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One vs all local model accuracy, (Client 0): 37.93103%\n",
      "One vs all local model accuracy, (Client 1): 39.58021%\n",
      "One vs all local model accuracy, (Client 2): 43.62819%\n",
      "One vs all local model accuracy, (Client 3): 41.07946%\n",
      "One vs all local model accuracy, (Client 4): 38.98051%\n",
      "One vs all local model accuracy, (Client 5): 35.08246%\n",
      "One vs all local model accuracy, (Client 6): 42.27886%\n",
      "One vs all local model accuracy, (Client 7): 41.97901%\n",
      "One vs all local model accuracy, (Client 8): 42.57871%\n",
      "One vs all local model accuracy, (Client 9): 40.17991%\n"
     ]
    }
   ],
   "source": [
    "################################# INDIVIDUAL CLIENTS XGBOOST REGRESSION MODEL TRAINING)\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Hyperparameters for each of the clients\n",
    "hyperparams = {\n",
    "    # \"objective\": \"reg:squarederror\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"n_estimators\": trees_client,\n",
    "    \"max_depth\": 5,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"base_score\": 0.5,  # np.mean(y_train)\n",
    "    \"random_state\": 34,\n",
    "}\n",
    "if num_classes ==2:\n",
    "    hyperparams[\"objective\"] = \"binary:logistic\"\n",
    "    \n",
    "for c, (x_train, y_train) in enumerate(\n",
    "        datasets\n",
    "):  # extract the dataset for the current client\n",
    "    reg = OneVsRestClassifier(xgb.XGBClassifier(**hyperparams)) # regression model training\n",
    "    reg.fit(x_train, y_train)\n",
    "    # save model\n",
    "    # note: there are two ways to save a one vs rest classifier model, either using joblib or pickle, json is not supported\n",
    "    checkpointpath = 'xgb_models/XGB_client_model_fed_{}.h5'.format(c)\n",
    "    joblib.dump(reg, checkpointpath, compress=0)\n",
    "  \n",
    "# test of the one vs all local model (compared with the previous case)\n",
    "    y_pred = reg.predict(x_valid)\n",
    "    error = accuracy_score(y_valid, y_pred)\n",
    "    cm = confusion_matrix(y_valid, y_pred)\n",
    "    print(f\"One vs all local model accuracy, (Client {c}): {100*error :.5f}%\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new \"dataset\" input to 1D-CNN which consists of the XGB trees regression models previously trained (locally):\n",
    "\n",
    "NOTE: During initialization, all xgboost models (of all clients) must be shared with all clients before starting the FL process. MQTT can be used for this (but also other methods apply). In the following xgboost base models are loaded from a shared folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting the data of client 0 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 1 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 2 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 3 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 4 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 5 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 6 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 7 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 8 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 9 ----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from utils import get_trees_predictions_xgb\n",
    "\n",
    "reshape_enabled = True # disable or enable the reshaping of xgboost outputs - \n",
    "# False: filter size = num_classes * trees_client, True filter_size = trees_client\n",
    "reshape_enabled = False\n",
    "#inputs_obj = \"binary\" \n",
    "\n",
    "if num_classes == 2:\n",
    "    reshape_enabled = False # always disabled for 2 classes (binary problem)\n",
    "\n",
    "inputs_obj = \"soft\"\n",
    "# other options: \n",
    "# objective = \"soft\" # applies a tanh activation to the xgboost tree soft outputs  \n",
    "# objective = \"binary\" # outputs of xgboost trees are binarized, \n",
    "\n",
    "# load all xgboost models and prepare the data\n",
    "XGB_models = []\n",
    "for c in range(num_clients):\n",
    "    if num_classes == 2:\n",
    "        checkpointpath = 'xgb_models/XGB_client_model_{}.h5'.format(c)\n",
    "    else:\n",
    "        checkpointpath1 = 'xgb_models/XGB_client_model_fed_{}.h5'.format(c)\n",
    "    xgb = joblib.load(checkpointpath1)\n",
    "    if num_classes == 2:\n",
    "        XGB_models.append(xgb)\n",
    "    else:\n",
    "        classifiers = xgb.estimators_\n",
    "        for q in range(num_classes):\n",
    "            XGB_models.append(classifiers [q])\n",
    "\n",
    "x_xgb_trees_out = []\n",
    "y_xgb_trees_out = []\n",
    "for c, (x_train, y_train) in enumerate(datasets):  # for each client\n",
    "    print(\"Converting the data of client\", c, 100 * \"-\")\n",
    "    # XGB trees outputs (for all XGBoost trees!) corresponding to training data of client c\n",
    "    # NOTE: numclasses is needed to clip the inputs\n",
    "    x_xgb_trees_out.append(get_trees_predictions_xgb(x_train, inputs_obj, *XGB_models, numclasses=num_classes, reshape_enabled=reshape_enabled)) \n",
    "    if num_classes == 2:\n",
    "        categorical_labels = y_train\n",
    "    else:\n",
    "        categorical_labels = np.squeeze(np.eye(num_classes)[y_train.reshape(-1)])\n",
    "    y_xgb_trees_out.append(categorical_labels) # true labels now categorical\n",
    "\n",
    "datasets_out = tuple(zip(x_xgb_trees_out, y_xgb_trees_out)) # dataset_out is the new federated dataset input to 1D-CNN (XGB trees output-> 1D-CNN -> accuracy)\n",
    "\n",
    "# Validation data\n",
    "\n",
    "xgb_valid_out = get_trees_predictions_xgb(x_valid, inputs_obj, *XGB_models, numclasses=num_classes, reshape_enabled=reshape_enabled) # XGB trees outputs corresponding to validation data: to simplify the reasoning, we apply same validation set for all (other options are also feasible)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FedXGBooost aggregator \n",
    "initialize the global model (or ensemble model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_2 (Conv1D)           (None, 37, 32)            1312      \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 1184)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 320)               379200    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 4)                 1284      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 381796 (1.46 MB)\n",
      "Trainable params: 381796 (1.46 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from models import CNN_mc # check the model in models.py\n",
    "\n",
    "filters = 32 # convolutional filters (32 ok, >32 too large, depends on tree structures) TO BE OPTIMIZED\n",
    "if reshape_enabled:\n",
    "    filter_size = trees_client\n",
    "else:\n",
    "    if num_classes == 2:\n",
    "        filter_size = trees_client # CNN filter size equal to the number of trees per client \n",
    "    else:\n",
    "        filter_size = trees_client * num_classes # CNN filter size equal to the number of trees per client * number of classes (if > 2)\n",
    "\n",
    "params_cnn = (num_clients, filter_size, trees_client, filters, num_classes)\n",
    "models_clients = []  # list of models\n",
    "\n",
    "model_global = CNN_mc(*params_cnn)  # global model\n",
    "num_layers = len(model_global.get_weights())\n",
    "\n",
    "model_global.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning process \n",
    "Federated Averaging with Adam optimizer simulator. No MQTT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0/25\n",
      "Round 1/25, Client 1/10\n",
      "Round 1/25, Client 2/10\n",
      "Round 1/25, Client 3/10\n",
      "Round 1/25, Client 4/10\n",
      "Round 1/25, Client 5/10\n",
      "Round 1/25, Client 6/10\n",
      "Round 1/25, Client 7/10\n",
      "Round 1/25, Client 8/10\n",
      "Round 1/25, Client 9/10\n",
      "Round 1/25, Client 10/10\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.9897 - accuracy: 0.6027\n",
      "Round 2/25, Client 1/10\n",
      "Round 2/25, Client 2/10\n",
      "Round 2/25, Client 3/10\n",
      "Round 2/25, Client 4/10\n",
      "Round 2/25, Client 5/10\n",
      "Round 2/25, Client 6/10\n",
      "Round 2/25, Client 7/10\n",
      "Round 2/25, Client 8/10\n",
      "Round 2/25, Client 9/10\n",
      "Round 2/25, Client 10/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m     model_client\u001b[38;5;241m.\u001b[39mset_weights(model_global\u001b[38;5;241m.\u001b[39mget_weights())  \n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# update phase\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     \u001b[43mmodel_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_train_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# train the model on the client data\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     models_clients\u001b[38;5;241m.\u001b[39mappend(model_client)  \u001b[38;5;66;03m# save the model\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# aggregation phase\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Amministratore\\anaconda3\\envs\\CNRalg4TRUSTroke\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Amministratore\\anaconda3\\envs\\CNRalg4TRUSTroke\\lib\\site-packages\\keras\\src\\engine\\training.py:1682\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1672\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cluster_coordinator \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1673\u001b[0m         tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mcoordinator\u001b[38;5;241m.\u001b[39mClusterCoordinator(\n\u001b[0;32m   1674\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\n\u001b[0;32m   1675\u001b[0m         )\n\u001b[0;32m   1676\u001b[0m     )\n\u001b[0;32m   1678\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mscope(), training_utils\u001b[38;5;241m.\u001b[39mRespectCompiledTrainableState(  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m   1679\u001b[0m     \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m   1680\u001b[0m ):\n\u001b[0;32m   1681\u001b[0m     \u001b[38;5;66;03m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[1;32m-> 1682\u001b[0m     data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1684\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1685\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1686\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1687\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1688\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1694\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1696\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1697\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1699\u001b[0m     \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   1700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mc:\\Users\\Amministratore\\anaconda3\\envs\\CNRalg4TRUSTroke\\lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1678\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1676\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorExactEvalDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1677\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Amministratore\\anaconda3\\envs\\CNRalg4TRUSTroke\\lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1285\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[0;32m   1282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution \u001b[38;5;241m=\u001b[39m steps_per_execution\n\u001b[0;32m   1284\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1285\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1289\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpss_evaluation_shards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpss_evaluation_shards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1301\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[0;32m   1303\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Amministratore\\anaconda3\\envs\\CNRalg4TRUSTroke\\lib\\site-packages\\keras\\src\\engine\\data_adapter.py:353\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m         flat_dataset \u001b[38;5;241m=\u001b[39m flat_dataset\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;241m1024\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(epochs)\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m flat_dataset\n\u001b[1;32m--> 353\u001b[0m indices_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mindices_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslice_batch_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    355\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslice_inputs(indices_dataset, inputs)\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Amministratore\\anaconda3\\envs\\CNRalg4TRUSTroke\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2323\u001b[0m, in \u001b[0;36mDatasetV2.flat_map\u001b[1;34m(self, map_func, name)\u001b[0m\n\u001b[0;32m   2319\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops -> flat_map_op ->\u001b[39;00m\n\u001b[0;32m   2320\u001b[0m \u001b[38;5;66;03m# dataset_ops).\u001b[39;00m\n\u001b[0;32m   2321\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[0;32m   2322\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m flat_map_op\n\u001b[1;32m-> 2323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mflat_map_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_map\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Amministratore\\anaconda3\\envs\\CNRalg4TRUSTroke\\lib\\site-packages\\tensorflow\\python\\data\\ops\\flat_map_op.py:24\u001b[0m, in \u001b[0;36m_flat_map\u001b[1;34m(input_dataset, map_func, name)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_flat_map\u001b[39m(input_dataset, map_func, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# pylint: disable=unused-private-name\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"See `Dataset.flat_map()` for details.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_FlatMapDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Amministratore\\anaconda3\\envs\\CNRalg4TRUSTroke\\lib\\site-packages\\tensorflow\\python\\data\\ops\\flat_map_op.py:33\u001b[0m, in \u001b[0;36m_FlatMapDataset.__init__\u001b[1;34m(self, input_dataset, map_func, name)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_dataset, map_func, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     32\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_dataset \u001b[38;5;241m=\u001b[39m input_dataset\n\u001b[1;32m---> 33\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func \u001b[38;5;241m=\u001b[39m \u001b[43mstructured_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStructuredFunctionWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformation_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39moutput_structure, dataset_ops\u001b[38;5;241m.\u001b[39mDatasetSpec):\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `map_func` argument must return a `Dataset` object. Got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_ops\u001b[38;5;241m.\u001b[39mget_type(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39moutput_structure)\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Amministratore\\anaconda3\\envs\\CNRalg4TRUSTroke\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:272\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m    265\u001b[0m       warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    266\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    267\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    268\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    269\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    270\u001b[0m     fn_factory \u001b[38;5;241m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function \u001b[38;5;241m=\u001b[39m \u001b[43mfn_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;66;03m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[0;32m    274\u001b[0m add_to_graph \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[1;32mc:\\Users\\Amministratore\\anaconda3\\envs\\CNRalg4TRUSTroke\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:1189\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1188\u001b[0m   \u001b[38;5;66;03m# Implements GenericFunction.get_concrete_function.\u001b[39;00m\n\u001b[1;32m-> 1189\u001b[0m   concrete \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1190\u001b[0m   concrete\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m concrete\n",
      "File \u001b[1;32mc:\\Users\\Amministratore\\anaconda3\\envs\\CNRalg4TRUSTroke\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:1169\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1167\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1168\u001b[0m     initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 1169\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_uninitialized_variables(initializers)\n\u001b[0;32m   1172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m   1173\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m   1174\u001b[0m   \u001b[38;5;66;03m# version which is guaranteed to never create variables.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Amministratore\\anaconda3\\envs\\CNRalg4TRUSTroke\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:694\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;66;03m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 694\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_fn\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_internal_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[0;32m    699\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Amministratore\\anaconda3\\envs\\CNRalg4TRUSTroke\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:176\u001b[0m, in \u001b[0;36mTracingCompiler._get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a concrete function which cleans up its graph function.\"\"\"\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m--> 176\u001b[0m   concrete_function, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_concrete_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[1;32mc:\\Users\\Amministratore\\anaconda3\\envs\\CNRalg4TRUSTroke\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:171\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_concrete_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m   args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[0;32m    169\u001b[0m   kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Amministratore\\anaconda3\\envs\\CNRalg4TRUSTroke\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:393\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    390\u001b[0m placeholder_context \u001b[38;5;241m=\u001b[39m trace_type\u001b[38;5;241m.\u001b[39mInternalPlaceholderContext(\n\u001b[0;32m    391\u001b[0m     func_graph, placeholder_mapping)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m func_graph\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[1;32m--> 393\u001b[0m   placeholder_bound_args \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_func_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplaceholder_arguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m      \u001b[49m\u001b[43mplaceholder_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m args \u001b[38;5;241m=\u001b[39m placeholder_bound_args\u001b[38;5;241m.\u001b[39margs\n\u001b[0;32m    396\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m placeholder_bound_args\u001b[38;5;241m.\u001b[39mkwargs\n",
      "File \u001b[1;32mc:\\Users\\Amministratore\\anaconda3\\envs\\CNRalg4TRUSTroke\\lib\\site-packages\\tensorflow\\core\\function\\polymorphism\\function_type.py:323\u001b[0m, in \u001b[0;36mFunctionType.placeholder_arguments\u001b[1;34m(self, placeholder_context)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not generate placeholder value for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartially defined function type.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    322\u001b[0m   placeholder_context\u001b[38;5;241m.\u001b[39mupdate_naming_scope(parameter\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m--> 323\u001b[0m   arguments[parameter\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m \u001b[43mparameter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype_constraint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplaceholder_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m      \u001b[49m\u001b[43mplaceholder_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mBoundArguments(\u001b[38;5;28mself\u001b[39m, arguments)\n",
      "File \u001b[1;32mc:\\Users\\Amministratore\\anaconda3\\envs\\CNRalg4TRUSTroke\\lib\\site-packages\\tensorflow\\python\\framework\\tensor.py:276\u001b[0m, in \u001b[0;36mTensorSpec.placeholder_value\u001b[1;34m(self, placeholder_context)\u001b[0m\n\u001b[0;32m    271\u001b[0m   placeholder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_placeholder(context_graph, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m   \u001b[38;5;66;03m# Record the requested/user-specified name in case it's different than\u001b[39;00m\n\u001b[0;32m    275\u001b[0m   \u001b[38;5;66;03m# the uniquified name, for validation when exporting signatures.\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m   \u001b[43mplaceholder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_attr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    277\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_user_specified_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattr_value_pb2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAttrValue\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m handle_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39m_handle_data  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    282\u001b[0m     handle_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m handle_data\u001b[38;5;241m.\u001b[39mis_set\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m handle_data\u001b[38;5;241m.\u001b[39mshape_and_type\n\u001b[0;32m    285\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Amministratore\\anaconda3\\envs\\CNRalg4TRUSTroke\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2220\u001b[0m, in \u001b[0;36mOperation._set_attr\u001b[1;34m(self, attr_name, attr_value)\u001b[0m\n\u001b[0;32m   2217\u001b[0m buf \u001b[38;5;241m=\u001b[39m pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_NewBufferFromString(\n\u001b[0;32m   2218\u001b[0m     compat\u001b[38;5;241m.\u001b[39mas_bytes(attr_value\u001b[38;5;241m.\u001b[39mSerializeToString()))\n\u001b[0;32m   2219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2220\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_attr_with_buf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattr_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2221\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   2222\u001b[0m   pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_DeleteBuffer(buf)\n",
      "File \u001b[1;32mc:\\Users\\Amministratore\\anaconda3\\envs\\CNRalg4TRUSTroke\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2228\u001b[0m, in \u001b[0;36mOperation._set_attr_with_buf\u001b[1;34m(self, attr_name, attr_buf)\u001b[0m\n\u001b[0;32m   2225\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Set an attr in the node_def with a pre-allocated buffer.\"\"\"\u001b[39;00m\n\u001b[0;32m   2226\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39m_c_graph\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;28;01mas\u001b[39;00m c_graph:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   2227\u001b[0m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 2228\u001b[0m   \u001b[43mpywrap_tf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSetAttr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr_buf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "R = 25  # global FL rounds\n",
    "E = 10  # local epochs\n",
    "\n",
    "print(f\"Round 0/{R}\")  # init model\n",
    "\n",
    "# sets labels to categorical\n",
    "if num_classes == 2:\n",
    "    categorical_y_valid = y_valid\n",
    "else:\n",
    "    categorical_y_valid = np.squeeze(np.eye(num_classes)[y_valid.reshape(-1)])\n",
    "\n",
    "for r in range(R):  # for each round\n",
    "    \n",
    "    # update phase for each client\n",
    "    for c, (x_train_c, y_train_c) in enumerate(datasets_out):  \n",
    "        print(f\"Round {r + 1}/{R}, Client {c + 1}/{num_clients}\")\n",
    "        model_client = CNN_mc(*params_cnn)  # create a new model\n",
    "        # set global weights (no memory of prev local weights)\n",
    "        model_client.set_weights(model_global.get_weights())  \n",
    "        # update phase\n",
    "        model_client.fit(\n",
    "            x_train_c, y_train_c, epochs=E, verbose=False\n",
    "        )  # train the model on the client data\n",
    "        models_clients.append(model_client)  # save the model\n",
    "    \n",
    "    # aggregation phase\n",
    "    global_weights = []\n",
    "    for i in range(num_layers):  # aggregate the weights, no memory of prev global weights\n",
    "        global_weights.append(\n",
    "            np.sum([model.get_weights()[i] for model in models_clients], axis=0)\n",
    "            / len(models_clients)\n",
    "        )\n",
    "    model_global.set_weights(global_weights)\n",
    "\n",
    "    model_global.evaluate(xgb_valid_out, categorical_y_valid)  # evaluate the global model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "\n",
    "y_hat_xgb = model_global.predict(xgb_valid_out)\n",
    "y_hat_xgb_cont = np.argmax(y_hat_xgb, axis=1)\n",
    "accuracy_fed = accuracy_score(y_valid, y_hat_xgb_cont)\n",
    "cm = confusion_matrix(y_valid, y_hat_xgb_cont)\n",
    "\n",
    "# performance and confusion matrix\n",
    "\n",
    "print(f\"Accuracy (Centralized): {accuracy_s :.2f}\")\n",
    "for c, error in enumerate(errors_clients):\n",
    "    print(f\"Accuracy (Client {c}): {error :.2f}\")\n",
    "print(f\"Accuracy (Federated): {accuracy_fed :.2f}\")\n",
    "\n",
    "# saving results\n",
    "checkpointpath = 'xgb_models/XGB_federated_model_regression_multiclass.keras'\n",
    "model_global.save(checkpointpath)\n",
    "dict_1 = {\n",
    "    \"Accuracy_centralized\": accuracy_s,\n",
    "    \"Accuracy_clients\": errors_clients,\n",
    "    \"Accuracy_federated\": accuracy_fed\n",
    "}\n",
    "sio.savemat(\n",
    "    \"results/fedXGboost_{}_features_{}_redundant_{}_classes_{}_clients_{}_trees_client_{}_train_samples_{}_reshape_{}_objective{}.mat\".format('iid',n_features,n_redundant,num_classes, num_clients, trees_client, samples, reshape_enabled, inputs_obj), dict_1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CNRalg4TRUSTroke",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
