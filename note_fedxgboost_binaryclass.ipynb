{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries \n",
    "Choose a dataset and set simulation parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "num_classes = 2 # only 2\n",
    "n_features = 50 # number of features in the dataset (TRUSTroke: 63)\n",
    "n_redundant = 5 # redundant features (TRUSTroke: unclear)\n",
    "n_informative = n_features - n_redundant # informative features\n",
    "test_size = 0.4 # fraction of data used for validation\n",
    "training_samples = 1000\n",
    "n_samples = round(training_samples/(1-test_size)) # total samples including also validation set and so that the fraction of data used for validation is test_size \n",
    "random_state = 42\n",
    "# load a tabular dataset for multiclass classification (example with scikitlearn datasets)\n",
    "X, y = datasets.make_classification(n_samples=n_samples, n_features=n_features, n_informative=n_informative, n_redundant=n_redundant, n_classes=num_classes)\n",
    "x_train, x_valid,  y_train, y_valid = train_test_split(X, y, test_size=test_size, random_state=random_state)    \n",
    "# save\n",
    "with open(f\"dataset/dataset_{num_classes}_redundant_{n_redundant}.pkl\", \"wb\") as f:\n",
    "    pickle.dump([x_train, x_valid,  y_train, y_valid], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets the number of clients, the training samples per client and the number of trees (xgboost) per client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clients = 10  # number of FL clients\n",
    "trees_client = 10  # number of xgboost trees per client\n",
    "samples = round(training_samples/num_clients) # number of training examples per client\n",
    "\n",
    "# load the dataset\n",
    "with open(f\"dataset/dataset_{num_classes}_redundant_{n_redundant}.pkl\", 'rb') as f:\n",
    "    x_train, x_valid,  y_train, y_valid = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centralized performance\n",
    "Data are fused on the server, this is the classical distributed xboost, privacy critical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['xgb_models/XGB_centralized_model.h5']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "from utils import accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "# xgboost parameters (example)\n",
    "hyperparams = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    # Same number of trees as in the decentralized case\n",
    "    \"n_estimators\": trees_client,\n",
    "    \"max_depth\": 5,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"base_score\": 0.5,\n",
    "    \"random_state\": 34,\n",
    "}\n",
    "    \n",
    "reg = xgb.XGBClassifier(**hyperparams)\n",
    "reg.fit(x_train, y_train)\n",
    "y_pred = reg.predict(x_valid)\n",
    "accuracy_s = accuracy_score(y_valid, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_s:.2f}\") \n",
    "\n",
    "cm = confusion_matrix(y_valid, y_pred)\n",
    "# save and store the centralized model\n",
    "checkpointpath1 = 'xgb_models/XGB_centralized_model.h5'\n",
    "joblib.dump(reg, checkpointpath1, compress=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolated case (no federation) \n",
    "Training of local xgboost models (base models of the ensemble)\n",
    "\n",
    "Code below implements iid/uniform data split among the deployed clients. It can be extened including sample/label/feature imbalance. Training and validation data are saved in different folders, namely data/client_i/train and data/client_i/validation. Parameter server parameters are also saved in the server folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting IID\n",
      "Client 0 | Samples 100\n",
      "Client 1 | Samples 100\n",
      "Client 2 | Samples 100\n",
      "Client 3 | Samples 100\n",
      "Client 4 | Samples 100\n",
      "Client 5 | Samples 100\n",
      "Client 6 | Samples 100\n",
      "Client 7 | Samples 100\n",
      "Client 8 | Samples 100\n",
      "Client 9 | Samples 100\n",
      "Saved train data\n",
      "Client 0 | Samples 667\n",
      "Client 1 | Samples 667\n",
      "Client 2 | Samples 667\n",
      "Client 3 | Samples 667\n",
      "Client 4 | Samples 667\n",
      "Client 5 | Samples 667\n",
      "Client 6 | Samples 667\n",
      "Client 7 | Samples 667\n",
      "Client 8 | Samples 667\n",
      "Client 9 | Samples 667\n",
      "Saved validation data\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "\n",
    "print('Splitting IID')\n",
    "local_size = samples  # uniform split, and a assign 'samples' samples\n",
    "# split the training dataset and create folders in data/client_#i/train\n",
    "for i in range(num_clients):\n",
    "    dir = f'data/client_{i}/train/' # create a folder with the local data for the client\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    start_index = i * local_size\n",
    "    end_index = (i + 1) * local_size\n",
    "    x_part = x_train[start_index:end_index]\n",
    "    y_part = y_train[start_index:end_index]\n",
    "        \n",
    "    print('Client {} | Samples {}'.format(i, len(y_part)))\n",
    "    np.save(dir + f'x_train.npy', x_part) # creating directories for train and validation\n",
    "    np.save(dir + f'y_train.npy', y_part)\n",
    "print(f'Saved train data')\n",
    "\n",
    "# split the validation dataset and create folders in data/client_#i/valid\n",
    "local_size = len(x_valid) // num_clients # validation data uniformly distributed across clients (other options are also possible) \n",
    "# local_size = len(x_valid)\n",
    "for i in range(num_clients):\n",
    "    dir = f'data/client_{i}/valid/' # create a folder with the local data for the client\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    start_index = i * local_size\n",
    "    end_index = (i + 1) * local_size\n",
    "    #x_part = x_valid[start_index:end_index] # uniform split of the validation set\n",
    "    #y_part = y_valid[start_index:end_index]\n",
    "    x_part = x_valid # all the clients have the same validation set (for fair comparison)\n",
    "    y_part = y_valid\n",
    "        \n",
    "    print('Client {} | Samples {}'.format(i, len(y_part)))\n",
    "    np.save(dir + f'x_valid.npy', x_part) # saving\n",
    "    np.save(dir + f'y_valid.npy', y_part)\n",
    "print(f'Saved validation data')\n",
    "\n",
    "\n",
    "x_train_clients = []\n",
    "y_train_clients = []\n",
    "x_valid_clients = []\n",
    "y_valid_clients = []\n",
    "\n",
    "# create train and valid datasets for all clients\n",
    "for k in range(num_clients):\n",
    "    x_train_clients.append(np.load(f'data/client_{k}/train/x_train.npy', allow_pickle=True))\n",
    "    y_train_clients.append(np.load(f'data/client_{k}/train/y_train.npy'))\n",
    "    x_valid_clients.append(np.load(f'data/client_{k}/valid/x_valid.npy', allow_pickle=True))\n",
    "    y_valid_clients.append(np.load(f'data/client_{k}/valid/y_valid.npy'))\n",
    "\n",
    "datasets = tuple(zip(x_train_clients, y_train_clients))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the xboost tree models locally. Decision tree models are the base models for fedxbgoostllr. Save the tree models and evaluate them separately (no federation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost classifier local model accuracy, (Client 0): 67.01649%\n",
      "xgboost classifier local model accuracy, (Client 1): 69.71514%\n",
      "xgboost classifier local model accuracy, (Client 2): 65.51724%\n",
      "xgboost classifier local model accuracy, (Client 3): 69.26537%\n",
      "xgboost classifier local model accuracy, (Client 4): 66.26687%\n",
      "xgboost classifier local model accuracy, (Client 5): 61.31934%\n",
      "xgboost classifier local model accuracy, (Client 6): 65.96702%\n",
      "xgboost classifier local model accuracy, (Client 7): 62.21889%\n",
      "xgboost classifier local model accuracy, (Client 8): 60.56972%\n",
      "xgboost classifier local model accuracy, (Client 9): 65.66717%\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for each of the clients\n",
    "hyperparams = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"n_estimators\": trees_client,\n",
    "    \"max_depth\": 5,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"base_score\": 0.5,  # np.mean(y_train)\n",
    "    \"random_state\": 34,\n",
    "}\n",
    "    \n",
    "errors_clients = []\n",
    "\n",
    "for c, (x_train, y_train) in enumerate(\n",
    "        datasets\n",
    "):  # extract the dataset for the current client\n",
    "    reg = xgb.XGBClassifier(**hyperparams) # train the classifier\n",
    "    reg.fit(x_train, y_train)\n",
    "    # save model\n",
    "    checkpointpath = 'xgb_models/XGB_client_model_{}.h5'.format(c)\n",
    "    joblib.dump(reg, checkpointpath, compress=0)\n",
    "    \n",
    "    # full performance tests (accuracy and confusion matrix)\n",
    "    y_pred = reg.predict(x_valid)\n",
    "    error = accuracy_score(y_valid, y_pred)\n",
    "    cm = confusion_matrix(y_valid, y_pred)\n",
    "    print(f\"xgboost classifier local model accuracy, (Client {c}): {100*error :.5f}%\")\n",
    "    errors_clients.append(error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Federated XGBoost: extraction of the local XGBoost tree outputs\n",
    "Create a new \"dataset\" which consists of the outputs of ALL the XGB trees models previously trained (ALL the local models)\n",
    "\n",
    "get_trees_predictions_xgb function is used to extract the individual trees prediction outputs from the local xgboost models (modify only if needed)\n",
    "\n",
    "This new dataset is the input to the global/ensemble model, which is a 1D-CNN. Global model is trained via FL. \n",
    "\n",
    "The general pipeline is the following (x_xgb_trees_out -> 1D-CNN -> predictions)\n",
    "\n",
    "NOTE: During initialization, all xgboost models (of all clients) must be shared with all clients before starting the FL process. MQTT can be used for this (but also other methods apply). In the following xgboost base models are loaded from a shared folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting the data of client 0 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 1 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 2 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 3 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 4 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 5 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 6 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 7 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 8 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 9 ----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from utils import get_trees_predictions_xgb\n",
    "\n",
    "reshape_enabled = False # disable or enable the reshaping of xgboost outputs - \n",
    "inputs_obj = \"soft\"\n",
    "# other options: \n",
    "# objective = \"soft\" # applies a tanh activation to the xgboost tree soft outputs  \n",
    "# objective = \"binary\" # outputs of xgboost trees are binarized, \n",
    "   \n",
    "# load all xgboost models and prepare the data\n",
    "XGB_models = []\n",
    "for c in range(num_clients):\n",
    "    checkpointpath = 'xgb_models/XGB_client_model_{}.h5'.format(c)\n",
    "    xgb = joblib.load(checkpointpath1)\n",
    "    XGB_models.append(xgb)\n",
    "    \n",
    "x_xgb_trees_out = []\n",
    "y_xgb_trees_out = []\n",
    "for c, (x_train, y_train) in enumerate(datasets):  # for each client\n",
    "    print(\"Converting the data of client\", c, 100 * \"-\")\n",
    "    # XGB trees outputs (for all XGBoost trees!) corresponding to training data of client c\n",
    "    x_xgb_trees_out.append(get_trees_predictions_xgb(x_train, inputs_obj, *XGB_models, numclasses=num_classes, reshape_enabled=reshape_enabled)) \n",
    "    y_xgb_trees_out.append(y_train) \n",
    "\n",
    "datasets_out = tuple(zip(x_xgb_trees_out, y_xgb_trees_out)) # dataset_out is the new federated dataset input to 1D-CNN (XGB trees output-> 1D-CNN -> accuracy)\n",
    "\n",
    "# Validation data (y_valid are the corresponding labels)\n",
    "xgb_valid_out = get_trees_predictions_xgb(x_valid, inputs_obj, *XGB_models, numclasses=num_classes, reshape_enabled=reshape_enabled) # XGB trees outputs corresponding to validation data: to simplify the reasoning, we apply same validation set for all (other options are also feasible)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global model (ensemble model): 1D CNN FedXGBooost aggregator \n",
    "initialize the global model (or ensemble model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_151\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_151 (Conv1D)         (None, 10, 32)            352       \n",
      "                                                                 \n",
      " flatten_151 (Flatten)       (None, 320)               0         \n",
      "                                                                 \n",
      " dense_302 (Dense)           (None, 320)               102720    \n",
      "                                                                 \n",
      " dense_303 (Dense)           (None, 1)                 321       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 103393 (403.88 KB)\n",
      "Trainable params: 103393 (403.88 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from models import CNN_mc # check the model in models.py\n",
    "\n",
    "filters = 32 # convolutional filters (32 ok, >32 too large, depends on tree structures) TO BE OPTIMIZED\n",
    "filter_size = trees_client # CNN filter size equal to the number of trees per client \n",
    "    \n",
    "params_cnn = (num_clients, filter_size, trees_client, filters, num_classes)\n",
    "models_clients = []  # list of models\n",
    "\n",
    "model_global = CNN_mc(*params_cnn)  # global model\n",
    "num_layers = len(model_global.get_weights())\n",
    "\n",
    "model_global.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning process (learning of 1D CNN model parameters)\n",
    "Federated Averaging with Adam optimizer simulator. No MQTT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0/15\n",
      "Round 1/15, Client 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1/15, Client 2/10\n",
      "Round 1/15, Client 3/10\n",
      "Round 1/15, Client 4/10\n",
      "Round 1/15, Client 5/10\n",
      "Round 1/15, Client 6/10\n",
      "Round 1/15, Client 7/10\n",
      "Round 1/15, Client 8/10\n",
      "Round 1/15, Client 9/10\n",
      "Round 1/15, Client 10/10\n"
     ]
    }
   ],
   "source": [
    "R = 15  # global FL rounds\n",
    "E = 10  # local epochs\n",
    "\n",
    "print(f\"Round 0/{R}\")  # init model\n",
    "\n",
    "for r in range(R):  # for each round\n",
    "    \n",
    "    # update phase for each client\n",
    "    for c, (x_train_c, y_train_c) in enumerate(datasets_out):  \n",
    "        print(f\"Round {r + 1}/{R}, Client {c + 1}/{num_clients}\")\n",
    "        model_client = CNN_mc(*params_cnn)  # create a new model\n",
    "        # set global weights (no memory of prev local weights)\n",
    "        model_client.set_weights(model_global.get_weights())  \n",
    "        # update phase\n",
    "        model_client.fit(\n",
    "            x_train_c, y_train_c, epochs=E, verbose=False\n",
    "        )  # train the model on the client data\n",
    "        models_clients.append(model_client)  # save the model\n",
    "    \n",
    "    # aggregation phase\n",
    "    global_weights = []\n",
    "    for i in range(num_layers):  # aggregate the weights, no memory of prev global weights\n",
    "        global_weights.append(\n",
    "            np.sum([model.get_weights()[i] for model in models_clients], axis=0)\n",
    "            / len(models_clients)\n",
    "        )\n",
    "    model_global.set_weights(global_weights)\n",
    "\n",
    "    model_global.evaluate(xgb_valid_out, y_valid)  # evaluate the global model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 1s 8ms/step\n",
      "Accuracy (Centralized): 0.78\n",
      "Accuracy (Client 0): 0.64\n",
      "Accuracy (Client 1): 0.61\n",
      "Accuracy (Client 2): 0.65\n",
      "Accuracy (Client 3): 0.61\n",
      "Accuracy (Client 4): 0.67\n",
      "Accuracy (Client 5): 0.67\n",
      "Accuracy (Client 6): 0.64\n",
      "Accuracy (Client 7): 0.65\n",
      "Accuracy (Client 8): 0.62\n",
      "Accuracy (Client 9): 0.63\n",
      "Accuracy (Federated): 0.75\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "\n",
    "y_hat_xgb = model_global.predict(xgb_valid_out)\n",
    "y_hat_xgb_cont = y_hat_xgb >= 0.5 # binary estimator (CNN model has sigmoid output)\n",
    "\n",
    "accuracy_fed = accuracy_score(y_valid, y_hat_xgb_cont)\n",
    "cm = confusion_matrix(y_valid, y_hat_xgb_cont)\n",
    "\n",
    "# performance and confusion matrix\n",
    "\n",
    "print(f\"Accuracy (Centralized): {accuracy_s :.2f}\")\n",
    "for c, error in enumerate(errors_clients):\n",
    "    print(f\"Accuracy (Client {c}): {error :.2f}\")\n",
    "print(f\"Accuracy (Federated): {accuracy_fed :.2f}\")\n",
    "\n",
    "# saving results\n",
    "checkpointpath = 'xgb_models/XGB_federated_model_regression_multiclass.keras'\n",
    "model_global.save(checkpointpath)\n",
    "dict_1 = {\n",
    "    \"Accuracy_centralized\": accuracy_s,\n",
    "    \"Accuracy_clients\": errors_clients,\n",
    "    \"Accuracy_federated\": accuracy_fed\n",
    "}\n",
    "sio.savemat(\n",
    "    \"results/fedXGboost_{}_features_{}_redundant_{}_classes_{}_clients_{}_trees_client_{}_train_samples_{}_reshape_{}_objective{}.mat\".format('iid',n_features,n_redundant,num_classes, num_clients, trees_client, samples, reshape_enabled, inputs_obj), dict_1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgboost_fed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
