{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries \n",
    "Choose a dataset and set simulation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.Datasets.data_loader import load_stroke, load_stroke_nprep\n",
    "# from classes.params import simul_param, fl_param\n",
    "\n",
    "\n",
    "data = \"kaggle_stroke\" # stroke data example 1\n",
    "run = 0\n",
    "\n",
    "# load the data (only for centralized perf)\n",
    "if data == \"kaggle_stroke\":\n",
    "    x_train, y_train, x_valid, y_valid = load_stroke()\n",
    "elif data == \"kaggle_stroke_nprep\":\n",
    "    x_train, y_train, x_valid, y_valid = load_stroke_nprep() # stroke with no SMOTE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets the number of clients and the number of trees (xgboost) per client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clients = 5  # K\n",
    "trees_client = 15  # M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centralized performance\n",
    "Data are fused on the server, this is the classical distributed xboost, privacy critical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Centralized), TPR, TNR: 85.14469 85.39095 84.73413%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['xgb_models/XGB_centralized_model.h5']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "from utils import accuracy\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "hyperparams = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    # Same number of trees as in the decentralized case\n",
    "    \"n_estimators\": num_clients * trees_client,\n",
    "    \"max_depth\": 5,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"base_score\": 0.5,\n",
    "    \"random_state\": 34,\n",
    "}\n",
    "\n",
    "reg = xgb.XGBClassifier(**hyperparams)\n",
    "reg.fit(x_train, y_train)\n",
    "y_pred = reg.predict(x_valid)\n",
    "\n",
    "error_centr = accuracy(y_valid, y_pred)\n",
    "cm = confusion_matrix(y_valid, y_pred)\n",
    "TPR_centralized = cm[1,1]/(cm[1,0] + cm[1,1])\n",
    "TNR_centralized = cm[0,0] / (cm[0,0] + cm[0,1])\n",
    "print(f\"Accuracy (Centralized), TPR, TNR: {100*error_centr :.5f} {100*TPR_centralized :.5f} {100*TNR_centralized :.5f}%\")\n",
    "# save and store the centralized model\n",
    "checkpointpath1 = 'xgb_models/XGB_centralized_model.h5'\n",
    "joblib.dump(reg, checkpointpath1, compress=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolated case (no federation) \n",
    "Training of local xgboost models (base models of the ensemble)\n",
    "\n",
    "Code below implements iid split (can be extened with sample/label/feature imbalance), saves training, validation data in data/client_i and server parameters in server folders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting IID\n",
      "Client 0 | Samples 100\n",
      "Client 1 | Samples 100\n",
      "Client 2 | Samples 100\n",
      "Client 3 | Samples 100\n",
      "Client 4 | Samples 100\n",
      "Saved train data\n",
      "Client 0 | Samples 311\n",
      "Client 1 | Samples 311\n",
      "Client 2 | Samples 311\n",
      "Client 3 | Samples 311\n",
      "Client 4 | Samples 311\n",
      "Saved valid data\n"
     ]
    }
   ],
   "source": [
    "from classes.Datasets.dataset_client import Dataset\n",
    "from classes.Datasets.data_partitioner import split_iid_sim\n",
    "import os, json\n",
    "# or run python -m classes.Datasets.data_generator.py to get a data distribution\n",
    "samples = 100\n",
    "niid_type = 'iid'\n",
    "alpha = 1\n",
    "\n",
    "print('Splitting IID')\n",
    "\n",
    "# split the training dataset and create folders in data/client_#i/train\n",
    "split_iid_sim(x_train, y_train, samples, num_clients, type='train')\n",
    "\n",
    "# split the validation dataset and create folders in data/client_#i/valid\n",
    "split_iid_sim(x_valid, y_valid, samples, num_clients, type='valid')\n",
    "\n",
    "## optional save data info to json for PS only\n",
    "# n_classes = np.unique(y_valid, axis=0).shape[0] if np.unique(y_valid, axis=0).shape[0]>2 else 1\n",
    "# data_info = {\n",
    "#        'input_shape': x_train.shape[1:],\n",
    "#        'num_classes': n_classes, #np.unique(y_valid, axis=0).shape[0],\n",
    "#        'data': data,\n",
    "#        'niid_type': niid_type,\n",
    "#        'alpha': alpha\n",
    "#    }\n",
    "# optional save data/server/\n",
    "# dir = \"data/server/\"\n",
    "# os.makedirs(dir, exist_ok=True)\n",
    "# with open(dir + \"data_info.json\", \"w\") as outfile:\n",
    "#    json.dump(data_info, outfile)\n",
    "\n",
    "x_train_clients = []\n",
    "y_train_clients = []\n",
    "x_valid_clients = []\n",
    "y_valid_clients = []\n",
    "\n",
    "# create train and valid datasets for all clients\n",
    "for k in range(num_clients):\n",
    "    handle = Dataset(k) # get an handle to training dataset of client k\n",
    "    x_train_clients.append(handle.x_train_local)\n",
    "    y_train_clients.append(handle.y_train_local)\n",
    "    x_valid_clients.append(handle.x_valid)\n",
    "    y_valid_clients.append(handle.y_valid)\n",
    "\n",
    "datasets = tuple(zip(x_train_clients, y_train_clients))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the xboost tree models locally. Decision tree models are the ensemble model (base models) for fedxbgoostllr. Save the ensembles and evaluate them separately (no federation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy, TPR, TNR (Client 0): 73.82637 78.80658 65.52316%\n",
      "Accuracy, TPR, TNR (Client 1): 70.54662 73.35391 65.86621%\n",
      "Accuracy, TPR, TNR (Client 2): 72.54019 78.80658 62.09262%\n",
      "Accuracy, TPR, TNR (Client 3): 71.70418 64.71193 83.36192%\n",
      "Accuracy, TPR, TNR (Client 4): 74.21222 81.99588 61.23499%\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for each of the clients\n",
    "hyperparams = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"n_estimators\": trees_client,\n",
    "    \"max_depth\": 5,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"base_score\": 0.5,  # np.mean(y_train)\n",
    "    \"random_state\": 34,\n",
    "}\n",
    "\n",
    "errors_clients = []\n",
    "TPR_clients = []\n",
    "TNR_clients = []\n",
    "for c, (x_train, y_train) in enumerate(\n",
    "        datasets\n",
    "):  # extract the dataset for the current client\n",
    "    reg = xgb.XGBClassifier(**hyperparams)\n",
    "    reg.fit(x_train, y_train)\n",
    "    # save model\n",
    "    checkpointpath = 'xgb_models/XGB_client_model_{}.h5'.format(c)\n",
    "    joblib.dump(reg, checkpointpath, compress=0)\n",
    "    \n",
    "    # full performance tests (accuracy and confusion matrix)\n",
    "    y_pred = reg.predict(x_valid)\n",
    "    error = accuracy(y_valid, y_pred)\n",
    "    cm = confusion_matrix(y_valid, y_pred)\n",
    "    TPR_isolated = cm[1,1] / (cm[1,0] + cm[1,1])\n",
    "    TNR_isolated = cm[0,0] / (cm[0,0] + cm[0,1])\n",
    "    print(f\"Accuracy, TPR, TNR (Client {c}): {100*error :.5f} {100*TPR_isolated :.5f} {100*TNR_isolated :.5f}%\")\n",
    "    errors_clients.append(error)\n",
    "    TPR_clients.append(TPR_isolated)\n",
    "    TNR_clients.append(TNR_isolated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated XGBoost \n",
    "The global model is a 1D-CNN type with specific filter sizes. The global model acts as an \"ensemble model\"\n",
    "\n",
    "The pipeline is the following (XGB trees outputs-> 1D-CNN -> predictions)\n",
    "\n",
    "Create FIRST a new \"dataset\" input to 1D-CNN which consists of XGB trees model outputs \n",
    "\n",
    "NOTE: During initialization, all xgboost models (of all clients) must be shared with all clients before starting the FL process. MQTT can be used for this (but also other methods apply). In the following xgboost base models are loaded from a shared folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting the data of client 0 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 1 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 2 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 3 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 4 ----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from utils import get_trees_predictions_xgb\n",
    "\n",
    "# load all xgboost models and prepare the data\n",
    "XGB_models = []\n",
    "for c in range(num_clients):\n",
    "    checkpointpath1 = 'xgb_models/XGB_client_model_{}.h5'.format(c)\n",
    "    xgb = joblib.load(checkpointpath1)\n",
    "    XGB_models.append(xgb)\n",
    "\n",
    "# prepare the new dataset for training\n",
    "objective = \"binary\"\n",
    "x_xgb_trees_out = []\n",
    "y_xgb_trees_out = []\n",
    "for c, (x_train, y_train) in enumerate(datasets):  # for each client\n",
    "    print(\"Converting the data of client\", c, 100 * \"-\")\n",
    "    x_xgb_trees_out.append(get_trees_predictions_xgb(x_train, objective, *XGB_models)) # XGB trees outputs (for all XGBoost trees!) corresponding to training data of client c\n",
    "    y_xgb_trees_out.append(y_train) # true labels of client c\n",
    "\n",
    "datasets_out = tuple(zip(x_xgb_trees_out, y_xgb_trees_out)) # dataset_out is the new federated dataset input to 1D-CNN (XGB trees output-> 1D-CNN -> accuracy)\n",
    "\n",
    "# Validation data\n",
    "\n",
    "xgb_valid_out = get_trees_predictions_xgb(x_valid, objective, *XGB_models) # XGB trees outputs corresponding to validation data: to simplify the reasoning, we apply same validation set for all (other options are also feasible)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FedXGBooost aggregator \n",
    "initialize the global model (or ensemble model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 5, 16)             256       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 80)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6817 (26.63 KB)\n",
      "Trainable params: 6817 (26.63 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from models import CNN\n",
    "\n",
    "filters = 16 # convolutional filters (16, 32 ok, >32 too large, depends on tree structures) TO BE OPTIMIZED\n",
    "filter_size = trees_client # CNN filter size MUST BE equal to the number of trees per client\n",
    "\n",
    "params_cnn = (num_clients, filter_size, filters, objective)\n",
    "models_clients = []  # list of models\n",
    "\n",
    "model_global = CNN(*params_cnn)  # global model\n",
    "num_layers = len(model_global.get_weights())\n",
    "\n",
    "model_global.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning process \n",
    "Federated Averaging with Adam optimizer simulator. No MQTT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0/6\n",
      "Round 1/6, Client 1/5\n",
      "Round 1/6, Client 2/5\n",
      "Round 1/6, Client 3/5\n",
      "Round 1/6, Client 4/5\n",
      "Round 1/6, Client 5/5\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.5956 - accuracy: 0.7357\n",
      "Round 2/6, Client 1/5\n",
      "Round 2/6, Client 2/5\n",
      "Round 2/6, Client 3/5\n",
      "Round 2/6, Client 4/5\n",
      "Round 2/6, Client 5/5\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.5286 - accuracy: 0.7640\n",
      "Round 3/6, Client 1/5\n",
      "Round 3/6, Client 2/5\n",
      "Round 3/6, Client 3/5\n",
      "Round 3/6, Client 4/5\n",
      "Round 3/6, Client 5/5\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.5462 - accuracy: 0.7595\n",
      "Round 4/6, Client 1/5\n",
      "Round 4/6, Client 2/5\n",
      "Round 4/6, Client 3/5\n",
      "Round 4/6, Client 4/5\n",
      "Round 4/6, Client 5/5\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.5596 - accuracy: 0.7621\n",
      "Round 5/6, Client 1/5\n",
      "Round 5/6, Client 2/5\n",
      "Round 5/6, Client 3/5\n",
      "Round 5/6, Client 4/5\n",
      "Round 5/6, Client 5/5\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.5603 - accuracy: 0.7633\n",
      "Round 6/6, Client 1/5\n",
      "Round 6/6, Client 2/5\n",
      "Round 6/6, Client 3/5\n",
      "Round 6/6, Client 4/5\n",
      "Round 6/6, Client 5/5\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.5666 - accuracy: 0.7666\n"
     ]
    }
   ],
   "source": [
    "R = 6  # global FL rounds\n",
    "E = 5  # local epochs\n",
    "\n",
    "print(f\"Round 0/{R}\")  # init model\n",
    "\n",
    "for r in range(R):  # for each round\n",
    "    \n",
    "    # update phase for each client\n",
    "    for c, (x_train_c, y_train_c) in enumerate(datasets_out):  \n",
    "        print(f\"Round {r + 1}/{R}, Client {c + 1}/{num_clients}\")\n",
    "        model_client = CNN(*params_cnn)  # create a new model\n",
    "        # set global weights (no memory of prev local weights)\n",
    "        model_client.set_weights(model_global.get_weights())  \n",
    "        # update phase\n",
    "        model_client.fit(\n",
    "            x_train_c, y_train_c, epochs=E, verbose=False\n",
    "        )  # train the model on the client data\n",
    "        models_clients.append(model_client)  # save the model\n",
    "    \n",
    "    # aggregation phase\n",
    "    global_weights = []\n",
    "    for i in range(num_layers):  # aggregate the weights, no memory of prev global weights\n",
    "        global_weights.append(\n",
    "            np.sum([model.get_weights()[i] for model in models_clients], axis=0)\n",
    "            / len(models_clients)\n",
    "        )\n",
    "    model_global.set_weights(global_weights)\n",
    "\n",
    "    model_global.evaluate(xgb_valid_out, y_valid)  # evaluate the global model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final testing on stroke data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 1s 1ms/step\n",
      "Accuracy (Centralized), TPR, TNR: 85.14469 85.39095 84.73413%\n",
      "Accuracy, TPR, TNR: (Client 0): 73.82637 78.80658 65.52316%\n",
      "Accuracy, TPR, TNR: (Client 1): 70.54662 73.35391 65.86621%\n",
      "Accuracy, TPR, TNR: (Client 2): 72.54019 78.80658 62.09262%\n",
      "Accuracy, TPR, TNR: (Client 3): 71.70418 64.71193 83.36192%\n",
      "Accuracy, TPR, TNR: (Client 4): 74.21222 81.99588 61.23499%\n",
      "Accuracy (Federated), TPR, TNR: 76.65595 84.25926 63.97942%\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "\n",
    "y_hat_xgbb = model_global.predict(xgb_valid_out)\n",
    "y_hat_xgb = y_hat_xgbb >= 0.5 # binary estimator (CNN model has sigmoid output)\n",
    "\n",
    "error_fed = accuracy(y_valid, y_hat_xgb)\n",
    "\n",
    "# performance and confusion matrix\n",
    "cm = confusion_matrix(y_valid, y_hat_xgb)\n",
    "TPR_fed = cm[1,1] / (cm[1,0] + cm[1,1])\n",
    "TNR_fed = cm[0,0] / (cm[0,0] + cm[0,1])\n",
    "\n",
    "\n",
    "print(f\"Accuracy (Centralized), TPR, TNR: {100*error_centr :.5f} {100*TPR_centralized :.5f} {100*TNR_centralized :.5f}%\")\n",
    "for c, error in enumerate(errors_clients):\n",
    "    print(f\"Accuracy, TPR, TNR: (Client {c}): {100*error :.5f} {100*TPR_clients[c] :.5f} {100*TNR_clients[c] :.5f}%\")\n",
    "print(f\"Accuracy (Federated), TPR, TNR: {100*error_fed :.5f} {100*TPR_fed :.5f} {100*TNR_fed :.5f}%\")\n",
    "\n",
    "# saving results\n",
    "# checkpointpath = 'xgb_models/XGB_federated_model.h5'\n",
    "# model_global.save(checkpointpath)\n",
    "## joblib.dump(model_global, checkpointpath, compress=0)\n",
    "# dict_1 = {\"Accuracy_centralized\": error_centr,\n",
    "#          \"TPR_centralized\":  TPR_centralized,\n",
    "#          \"TNR_centralized\":  TNR_centralized,\n",
    "#          \"Accuracy_clients\": errors_clients,\n",
    "#          \"TPR_clients\": TPR_clients,\n",
    "#          \"TNR_clients\": TNR_clients,\n",
    "#          \"Accuracy_federation\": error_fed,\n",
    "#          \"TPR_federation\": TPR_fed,\n",
    "#          \"TNR_federation\": TNR_fed,\n",
    "#\n",
    "#          }\n",
    "# sio.savemat(\n",
    "#    \"results/fedXGboost_{}_alpha_{}_samples_{}_run_{}.mat\".format('iid',0,100,run), dict_1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgboost_fed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
