{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries \n",
    "Choose a dataset and set simulation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from classes.params import simul_param, fl_param\n",
    "\n",
    "# load a tabular dataset (example with scikitlearn datasets)\n",
    "X, y = datasets.make_regression(n_samples=1000, n_features=10, n_informative=7, n_targets=1)\n",
    "x_train, x_valid,  y_train, y_valid = train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets the number of clients and the number of trees (xgboost) per client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clients = 5  # K\n",
    "trees_client = 10 # M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centralized performance\n",
    "Data are fused on the server, this is the classical distributed xgboost, privacy critical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 2644.02\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['xgb_models/XGB_centralized_model.h5']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "hyperparams = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    # Same number of trees as in the decentralized case\n",
    "    \"n_estimators\": num_clients * trees_client,\n",
    "    \"max_depth\": 5,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"base_score\": 0.5,\n",
    "    \"random_state\": 34,\n",
    "}\n",
    "\n",
    "reg = xgb.XGBRegressor(**hyperparams)\n",
    "reg.fit(x_train, y_train)\n",
    "y_pred = reg.predict(x_valid)\n",
    "error_centr = mean_squared_error(y_valid, y_pred)\n",
    "\n",
    "print(f\"MSE: {error_centr:.2f}\") \n",
    "\n",
    "# save and store the centralized model\n",
    "checkpointpath1 = 'xgb_models/XGB_centralized_model.h5'\n",
    "joblib.dump(reg, checkpointpath1, compress=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolated case (no federation) \n",
    "Training of local xgboost models (base models of the ensemble)\n",
    "\n",
    "Code below implements iid split (can be extened with sample/label/feature imbalance), saves training, validation data in data/client_i and server parameters in server folders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting IID\n",
      "Client 0 | Samples 100\n",
      "Client 1 | Samples 100\n",
      "Client 2 | Samples 100\n",
      "Client 3 | Samples 100\n",
      "Client 4 | Samples 100\n",
      "Saved train data\n",
      "Client 0 | Samples 80\n",
      "Client 1 | Samples 80\n",
      "Client 2 | Samples 80\n",
      "Client 3 | Samples 80\n",
      "Client 4 | Samples 80\n",
      "Saved valid data\n"
     ]
    }
   ],
   "source": [
    "from classes.Datasets.dataset_client import Dataset\n",
    "from classes.Datasets.data_partitioner import split_iid_sim\n",
    "import os, json\n",
    "# or run python -m classes.Datasets.data_generator.py to get a data distribution\n",
    "samples = 100\n",
    "niid_type = 'iid'\n",
    "alpha = 1\n",
    "\n",
    "print('Splitting IID')\n",
    "\n",
    "# split the training dataset and create folders in data/client_#i/train\n",
    "split_iid_sim(x_train, y_train, samples, num_clients, type='train')\n",
    "\n",
    "# split the validation dataset and create folders in data/client_#i/valid\n",
    "split_iid_sim(x_valid, y_valid, samples, num_clients, type='valid')\n",
    "\n",
    "x_train_clients = []\n",
    "y_train_clients = []\n",
    "x_valid_clients = []\n",
    "y_valid_clients = []\n",
    "\n",
    "# create train and valid datasets for all clients\n",
    "for k in range(num_clients):\n",
    "    handle = Dataset(k) # get an handle to training dataset of client k\n",
    "    x_train_clients.append(handle.x_train_local)\n",
    "    y_train_clients.append(handle.y_train_local)\n",
    "    x_valid_clients.append(handle.x_valid)\n",
    "    y_valid_clients.append(handle.y_valid)\n",
    "\n",
    "datasets = tuple(zip(x_train_clients, y_train_clients))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the xboost tree models locally. Decision tree models are the ensemble model (base models) for fedxbgoostllr. Save the ensembles and evaluate them separately (no federation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE, (Client 0): 10936.35\n",
      "MSE, (Client 1): 12367.12\n",
      "MSE, (Client 2): 11425.08\n",
      "MSE, (Client 3): 11698.99\n",
      "MSE, (Client 4): 11588.88\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for each of the clients\n",
    "hyperparams = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"n_estimators\": trees_client,\n",
    "    \"max_depth\": 5,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"base_score\": 0.5,  # np.mean(y_train)\n",
    "    \"random_state\": 34,\n",
    "}\n",
    "\n",
    "errors_clients = []\n",
    "for c, (x_train, y_train) in enumerate(\n",
    "        datasets\n",
    "):  # extract the dataset for the current client\n",
    "    reg = xgb.XGBRegressor(**hyperparams) # train the model\n",
    "    reg.fit(x_train, y_train)\n",
    "    # save model\n",
    "    checkpointpath = 'xgb_models/XGB_client_model_{}.h5'.format(c)\n",
    "    joblib.dump(reg, checkpointpath, compress=0)\n",
    "    \n",
    "    # MSE test\n",
    "    y_pred = reg.predict(x_valid)\n",
    "    error = mean_squared_error(y_valid, y_pred)\n",
    "    print(f\"MSE, (Client {c}): {error :.2f}\")\n",
    "    errors_clients.append(error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated XGBoost \n",
    "The global model is a 1D-CNN type with specific filter sizes. The global model acts as an \"ensemble model\"\n",
    "\n",
    "The pipeline is the following (XGB trees outputs-> 1D-CNN -> predictions)\n",
    "\n",
    "Create FIRST a new \"dataset\" input to 1D-CNN which consists of XGB trees model outputs \n",
    "\n",
    "NOTE: During initialization, all xgboost models (of all clients) must be shared with all clients before starting the FL process. MQTT can be used for this (but also other methods apply). In the following xgboost base models are loaded from a shared folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting the data of client 0 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 1 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 2 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 3 ----------------------------------------------------------------------------------------------------\n",
      "Converting the data of client 4 ----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from utils import get_trees_predictions_xgb\n",
    "\n",
    "# load all xgboost models and prepare the data\n",
    "XGB_models = []\n",
    "for c in range(num_clients):\n",
    "    checkpointpath1 = 'xgb_models/XGB_client_model_{}.h5'.format(c)\n",
    "    xgb = joblib.load(checkpointpath1)\n",
    "    XGB_models.append(xgb)\n",
    "\n",
    "# prepare the new dataset for training\n",
    "objective = \"regression\"\n",
    "x_xgb_trees_out = []\n",
    "y_xgb_trees_out = []\n",
    "for c, (x_train, y_train) in enumerate(datasets):  # for each client\n",
    "    print(\"Converting the data of client\", c, 100 * \"-\")\n",
    "    x_xgb_trees_out.append(get_trees_predictions_xgb(x_train, objective, *XGB_models)) # XGB trees outputs (for all XGBoost trees!) corresponding to training data of client c\n",
    "    y_xgb_trees_out.append(y_train) # true labels of client c\n",
    "\n",
    "datasets_out = tuple(zip(x_xgb_trees_out, y_xgb_trees_out)) # dataset_out is the new federated dataset input to 1D-CNN (XGB trees output-> 1D-CNN -> accuracy)\n",
    "\n",
    "# Validation data\n",
    "\n",
    "xgb_valid_out = get_trees_predictions_xgb(x_valid, objective, *XGB_models) # XGB trees outputs corresponding to validation data: to simplify the reasoning, we apply same validation set for all (other options are also feasible)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FedXGBooost aggregator \n",
    "initialize the global model (or ensemble model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_1 (Conv1D)           (None, 5, 32)             352       \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 160)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 160)               25760     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 161       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26273 (102.63 KB)\n",
      "Trainable params: 26273 (102.63 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from models import CNN\n",
    "\n",
    "filters = 32 # convolutional filters (16, 32 ok, >32 too large, depends on tree structures) TO BE OPTIMIZED\n",
    "filter_size = trees_client # CNN filter size MUST BE equal to the number of trees per client\n",
    "\n",
    "params_cnn = (num_clients, filter_size, filters, objective)\n",
    "models_clients = []  # list of models\n",
    "\n",
    "model_global = CNN(*params_cnn)  # global model\n",
    "num_layers = len(model_global.get_weights())\n",
    "\n",
    "model_global.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning process \n",
    "Federated Averaging with Adam optimizer simulator. No MQTT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0/25\n",
      "Round 1/25, Client 1/5\n",
      "Round 1/25, Client 2/5\n",
      "Round 1/25, Client 3/5\n",
      "Round 1/25, Client 4/5\n",
      "Round 1/25, Client 5/5\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 4008.6819\n",
      "Round 2/25, Client 1/5\n",
      "Round 2/25, Client 2/5\n",
      "Round 2/25, Client 3/5\n",
      "Round 2/25, Client 4/5\n",
      "Round 2/25, Client 5/5\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3969.6101\n",
      "Round 3/25, Client 1/5\n",
      "Round 3/25, Client 2/5\n",
      "Round 3/25, Client 3/5\n",
      "Round 3/25, Client 4/5\n",
      "Round 3/25, Client 5/5\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 4049.2334\n",
      "Round 4/25, Client 1/5\n",
      "Round 4/25, Client 2/5\n",
      "Round 4/25, Client 3/5\n",
      "Round 4/25, Client 4/5\n",
      "Round 4/25, Client 5/5\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3989.3228\n",
      "Round 5/25, Client 1/5\n",
      "Round 5/25, Client 2/5\n",
      "Round 5/25, Client 3/5\n",
      "Round 5/25, Client 4/5\n",
      "Round 5/25, Client 5/5\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 3963.5090\n",
      "Round 6/25, Client 1/5\n",
      "Round 6/25, Client 2/5\n",
      "Round 6/25, Client 3/5\n",
      "Round 6/25, Client 4/5\n",
      "Round 6/25, Client 5/5\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 3870.3682\n",
      "Round 7/25, Client 1/5\n",
      "Round 7/25, Client 2/5\n",
      "Round 7/25, Client 3/5\n",
      "Round 7/25, Client 4/5\n",
      "Round 7/25, Client 5/5\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3874.9573\n",
      "Round 8/25, Client 1/5\n",
      "Round 8/25, Client 2/5\n",
      "Round 8/25, Client 3/5\n",
      "Round 8/25, Client 4/5\n",
      "Round 8/25, Client 5/5\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3856.7905\n",
      "Round 9/25, Client 1/5\n",
      "Round 9/25, Client 2/5\n",
      "Round 9/25, Client 3/5\n",
      "Round 9/25, Client 4/5\n",
      "Round 9/25, Client 5/5\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3841.1064\n",
      "Round 10/25, Client 1/5\n",
      "Round 10/25, Client 2/5\n",
      "Round 10/25, Client 3/5\n",
      "Round 10/25, Client 4/5\n",
      "Round 10/25, Client 5/5\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3821.4924\n",
      "Round 11/25, Client 1/5\n",
      "Round 11/25, Client 2/5\n",
      "Round 11/25, Client 3/5\n",
      "Round 11/25, Client 4/5\n",
      "Round 11/25, Client 5/5\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3850.4307\n",
      "Round 12/25, Client 1/5\n",
      "Round 12/25, Client 2/5\n",
      "Round 12/25, Client 3/5\n",
      "Round 12/25, Client 4/5\n",
      "Round 12/25, Client 5/5\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3822.0527\n",
      "Round 13/25, Client 1/5\n",
      "Round 13/25, Client 2/5\n",
      "Round 13/25, Client 3/5\n",
      "Round 13/25, Client 4/5\n",
      "Round 13/25, Client 5/5\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3822.2043\n",
      "Round 14/25, Client 1/5\n",
      "Round 14/25, Client 2/5\n",
      "Round 14/25, Client 3/5\n",
      "Round 14/25, Client 4/5\n",
      "Round 14/25, Client 5/5\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3816.5347\n",
      "Round 15/25, Client 1/5\n",
      "Round 15/25, Client 2/5\n",
      "Round 15/25, Client 3/5\n",
      "Round 15/25, Client 4/5\n",
      "Round 15/25, Client 5/5\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3812.8694\n",
      "Round 16/25, Client 1/5\n",
      "Round 16/25, Client 2/5\n",
      "Round 16/25, Client 3/5\n",
      "Round 16/25, Client 4/5\n",
      "Round 16/25, Client 5/5\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3829.9312\n",
      "Round 17/25, Client 1/5\n",
      "Round 17/25, Client 2/5\n",
      "Round 17/25, Client 3/5\n",
      "Round 17/25, Client 4/5\n",
      "Round 17/25, Client 5/5\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3809.1787\n",
      "Round 18/25, Client 1/5\n",
      "Round 18/25, Client 2/5\n",
      "Round 18/25, Client 3/5\n",
      "Round 18/25, Client 4/5\n",
      "Round 18/25, Client 5/5\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3804.1946\n",
      "Round 19/25, Client 1/5\n",
      "Round 19/25, Client 2/5\n",
      "Round 19/25, Client 3/5\n",
      "Round 19/25, Client 4/5\n",
      "Round 19/25, Client 5/5\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3815.2097\n",
      "Round 20/25, Client 1/5\n",
      "Round 20/25, Client 2/5\n",
      "Round 20/25, Client 3/5\n",
      "Round 20/25, Client 4/5\n",
      "Round 20/25, Client 5/5\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3813.5632\n",
      "Round 21/25, Client 1/5\n",
      "Round 21/25, Client 2/5\n",
      "Round 21/25, Client 3/5\n",
      "Round 21/25, Client 4/5\n",
      "Round 21/25, Client 5/5\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3797.7344\n",
      "Round 22/25, Client 1/5\n",
      "Round 22/25, Client 2/5\n",
      "Round 22/25, Client 3/5\n",
      "Round 22/25, Client 4/5\n",
      "Round 22/25, Client 5/5\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3792.2102\n",
      "Round 23/25, Client 1/5\n",
      "Round 23/25, Client 2/5\n",
      "Round 23/25, Client 3/5\n",
      "Round 23/25, Client 4/5\n",
      "Round 23/25, Client 5/5\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3804.7681\n",
      "Round 24/25, Client 1/5\n",
      "Round 24/25, Client 2/5\n",
      "Round 24/25, Client 3/5\n",
      "Round 24/25, Client 4/5\n",
      "Round 24/25, Client 5/5\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 3800.5579\n",
      "Round 25/25, Client 1/5\n",
      "Round 25/25, Client 2/5\n",
      "Round 25/25, Client 3/5\n",
      "Round 25/25, Client 4/5\n",
      "Round 25/25, Client 5/5\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 3797.6707\n"
     ]
    }
   ],
   "source": [
    "R = 25  # global FL rounds\n",
    "E = 10  # local epochs (local training of xgboost models)\n",
    "\n",
    "print(f\"Round 0/{R}\")  # init model\n",
    "\n",
    "for r in range(R):  # for each round\n",
    "    \n",
    "    # update phase for each client\n",
    "    for c, (x_train_c, y_train_c) in enumerate(datasets_out):  \n",
    "        print(f\"Round {r + 1}/{R}, Client {c + 1}/{num_clients}\")\n",
    "        model_client = CNN(*params_cnn)  # create a new model\n",
    "        # set global weights (no memory of prev local weights)\n",
    "        model_client.set_weights(model_global.get_weights())  \n",
    "        # update phase\n",
    "        model_client.fit(\n",
    "            x_train_c, y_train_c, epochs=E, verbose=False\n",
    "        )  # train the model on the client data\n",
    "        models_clients.append(model_client)  # save the model\n",
    "    \n",
    "    # aggregation phase\n",
    "    global_weights = []\n",
    "    for i in range(num_layers):  # aggregate the weights, no memory of prev global weights\n",
    "        global_weights.append(\n",
    "            np.sum([model.get_weights()[i] for model in models_clients], axis=0)\n",
    "            / len(models_clients)\n",
    "        )\n",
    "    model_global.set_weights(global_weights)\n",
    "\n",
    "    model_global.evaluate(xgb_valid_out, y_valid)  # evaluate the global model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 2ms/step\n",
      "MSE (Centralized): 2644.02\n",
      "MSE (Client 0): 10936.35\n",
      "MSE (Client 1): 12367.12\n",
      "MSE (Client 2): 11425.08\n",
      "MSE (Client 3): 11698.99\n",
      "MSE (Client 4): 11588.88\n",
      "MSE (Federated): 3797.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\xgboost_fed\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "\n",
    "y_hat_xgb = model_global.predict(xgb_valid_out)\n",
    "error_fed = mean_squared_error(y_valid, y_hat_xgb)\n",
    "\n",
    "# performance and mse\n",
    "print(f\"MSE (Centralized): {error_centr :.2f}\")\n",
    "for c, error in enumerate(errors_clients):\n",
    "    print(f\"MSE (Client {c}): {error :.2f}\")\n",
    "print(f\"MSE (Federated): {error_fed :.2f}\")\n",
    "\n",
    "# saving results\n",
    "checkpointpath = 'xgb_models/XGB_federated_model_regression.h5'\n",
    "model_global.save(checkpointpath)\n",
    "# joblib.dump(model_global, checkpointpath, compress=0)\n",
    "dict_1 = {\"MSE_centralized\": error_centr,\n",
    "          \"MSE_clients\": errors_clients,\n",
    "          \"MSE_federation\": error_fed,\n",
    "          }\n",
    "sio.savemat(\n",
    "    \"results/fedXGboost_regression.mat\", dict_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgboost_fed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
